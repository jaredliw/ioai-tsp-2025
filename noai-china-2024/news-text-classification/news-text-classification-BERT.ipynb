{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d29bfd-a73f-4ce2-a181-8dbd56b76aa1",
   "metadata": {},
   "source": [
    "# News Text Classification Task\n",
    "\n",
    "> BERT Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e57a15-a26f-40b8-af1e-7f4b396e0bcc",
   "metadata": {},
   "source": [
    "> **Introduction:** This is the Question 3 of APOAI 2025 Mock Competition, and it is also the third question of the NOAI 2024 (China).\n",
    "\n",
    "## I. Question Overview\n",
    "\n",
    "A dataset for news text classification is provided, which is stored in a .csv file and contains two variables:\n",
    "\n",
    "- `text`: The content of the news text.\n",
    "- `category`: The category of the news text.\n",
    "\n",
    "The training set is stored in `train_news.csv`, with a total of 1,000 samples. The testing set is stored in `test_news.csv`, with a total of 200 samples. During the competition, the test set samples without labels will be provided.\n",
    "\n",
    "## II. Data Set\n",
    "\n",
    "1. Address of the training set: `train_news.csv`, [Training Set](https://bohrium.dp.tech/competitions/2223242868?tab=datasets);\n",
    "2. Test set (without labels): `test_news_nolabel.csv`, which contestants cannot access or download;\n",
    "3. Test set (with labels): `test_news_label.csv`, which contestants cannot access or download.\n",
    "\n",
    "## III. Task\n",
    "\n",
    "Please use PyTorch to design and train a natural language processing model to achieve the news text classification task, that is, input the sentences of the news and output the news categories.\n",
    "\n",
    "The specific requirements are as follows:\n",
    "\n",
    "1. The total training time and testing time using the CPU should not exceed 10 minutes. The connection time and queuing time are not counted into the total time.\n",
    "2. Tip: It is recommended to use Word Embedding + LSTM.\n",
    "\n",
    "## IV. Submission\n",
    "\n",
    "Please submit the `submission.ipynb` file, which contains the entire process of training the model. In `submission.ipynb`, store the prediction results of the test set in `submission.csv`. The naming and storage method of the label should be consistent with that of `train_news.csv`.\n",
    "\n",
    "You can refer to the submission format in `baseline.ipynb`. The address of `baseline.ipynb`: [Question 3 of APOAI Mock Competition_baseline](https://bohrium.dp.tech/notebooks/84584239178).\n",
    "\n",
    "## V. Scoring\n",
    "\n",
    "1. When the training and testing are completed within the specified time, the scoring criterion is the average value of the F1-Scores of all categories. Please look up the meaning of F1-Score on the Internet by yourself.\n",
    "2. If the F1-Scores of all categories are not calculated, a score of 0 will be given.\n",
    "3. If the total time for training and testing exceeds the time limit, a score of 0 will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbed7dd0-10de-45fc-af43-c55a7846a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "TEST_PATH = Path(os.environ.get(\"DATA_PATH\") or \"\")  # For grader\n",
    "TRAIN_PATH = Path(\"\")\n",
    "if TEST_PATH != TRAIN_PATH:\n",
    "    TRAIN_PATH = Path(\"/bohr/train-t05i/v2\")\n",
    "\n",
    "nltk.data.path.append(TRAIN_PATH / \"punkt\")\n",
    "\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cedad67-a7a1-4467-bbff-534b1ce9047e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Campbell rescues Arsenal\\n\\nSol Campbell prove...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria hit by further gas riots\\n\\nAlgeria su...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Fannie Mae bosses resign\\n\\nThe two mos...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia gets investment blessing\\n\\nSoaring oil...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Injury sidelines Philippoussis\\n\\nMark Philipp...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Roxy Music on Isle of Wight bill\\n\\nRoxy Music...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Qwest may spark MCI bidding war\\n\\nUS phone co...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Newcastle 2-1 Bolton\\n\\nKieron Dyer smashed ho...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Format wars could 'confuse users'\\n\\nTechnolog...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Clyde 0-5 Celtic\\n\\nCeltic brushed aside Clyde...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       category\n",
       "0    Campbell rescues Arsenal\\n\\nSol Campbell prove...          sport\n",
       "1    Algeria hit by further gas riots\\n\\nAlgeria su...       business\n",
       "2    Senior Fannie Mae bosses resign\\n\\nThe two mos...       business\n",
       "3    Russia gets investment blessing\\n\\nSoaring oil...       business\n",
       "4    Injury sidelines Philippoussis\\n\\nMark Philipp...          sport\n",
       "..                                                 ...            ...\n",
       "995  Roxy Music on Isle of Wight bill\\n\\nRoxy Music...  entertainment\n",
       "996  Qwest may spark MCI bidding war\\n\\nUS phone co...       business\n",
       "997  Newcastle 2-1 Bolton\\n\\nKieron Dyer smashed ho...          sport\n",
       "998  Format wars could 'confuse users'\\n\\nTechnolog...           tech\n",
       "999  Clyde 0-5 Celtic\\n\\nCeltic brushed aside Clyde...          sport\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH / \"train_news.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ae15c-3bd8-451b-8294-fa3d1ec29ce9",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab03cf5-089f-4a56-a1e1-943fc22c1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, vocab=None, *, max_len=500, vocab_size=25000):\n",
    "    # Tokenize\n",
    "    text_tokens = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab_provided = vocab is not None\n",
    "    if not vocab_provided:\n",
    "        common_words = Counter([token for text in text_tokens for token in text]).most_common(vocab_size - 2)\n",
    "        vocab = {word: idx + 2 for idx, (word, _) in enumerate(common_words)}\n",
    "        vocab[\"<UNK>\"] = 1\n",
    "        vocab[\"<PAD>\"] = 0\n",
    "\n",
    "    # Tokens to token IDs\n",
    "    text_token_ids = []\n",
    "    for text in text_tokens:\n",
    "        encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in text]\n",
    "        # Truncate if more, pad if less\n",
    "        encoded += [vocab[\"<PAD>\"]] * (max_len - len(encoded))\n",
    "        encoded = encoded[:max_len]\n",
    "        text_token_ids.append(encoded)\n",
    "\n",
    "    if vocab_provided:\n",
    "        return text_token_ids\n",
    "    return text_token_ids, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5a86ec-b772-41b3-9cd4-c47b636cb291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'sport', 'tech'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(df_train[\"category\"])\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e897972-4b6b-4a8a-addb-b340320f9446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23371"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, vocab = preprocess(df_train[\"text\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377f82c9-8a14-465e-830d-2814724c7b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx] if self.labels is not None else self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74956d6-520e-479b-a0eb-e5f359eb96be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_train = TextDataset(X_train, y_train)\n",
    "dl_train = DataLoader(ds_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0f346-385a-4ab4-8a42-2df240d5551f",
   "metadata": {},
   "source": [
    "## Define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f512db5-11c2-4c5b-bddc-bdab3cd2bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, dim_feedforward, num_layers, num_classes, padding_idx=0, max_len=500):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.cls_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        padding_mask = (x == self.token_embedding.padding_idx)\n",
    "        x = self.token_embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        encoded = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        cls_token = encoded[:, 0, :]\n",
    "\n",
    "        logits = self.cls_head(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b02da8-75be-4063-8ea2-9bd96953476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, criterion, dataloader, num_epochs):\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        running_loss /= len(dataloader.dataset)\n",
    "        train_losses.append(running_loss)\n",
    "        accuracy = eval(model, device, dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss}, Accuracy: {accuracy}\")\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c189b99-904f-4c1a-abfb-77964ccbd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, device, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        preds = torch.argmax(outputs, axis=1)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "    return torch.hstack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d509018f-745e-4f4b-a375-17a0254c7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, device, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        preds = predict(model, device, [(inputs, None)])\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5d1d2d-5355-4361-8e3e-c4eeee7ce21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "dim_feedforward = 64\n",
    "num_layers = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MyModel(len(vocab), embedding_dim, num_heads, dim_feedforward, num_layers, len(label_encoder.classes_), vocab[\"<PAD>\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5494764-df88-4287-8a1c-865870d14644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 1.4124162874221802, Accuracy: 0.376\n",
      "Epoch [2/12], Loss: 1.3173480281829835, Accuracy: 0.455\n",
      "Epoch [3/12], Loss: 1.2213904294967652, Accuracy: 0.542\n",
      "Epoch [4/12], Loss: 1.0598759880065918, Accuracy: 0.649\n",
      "Epoch [5/12], Loss: 0.8146404070854187, Accuracy: 0.761\n",
      "Epoch [6/12], Loss: 0.6225976181030274, Accuracy: 0.858\n",
      "Epoch [7/12], Loss: 0.44302326774597167, Accuracy: 0.923\n",
      "Epoch [8/12], Loss: 0.2919062526226044, Accuracy: 0.969\n",
      "Epoch [9/12], Loss: 0.1954629322886467, Accuracy: 0.975\n",
      "Epoch [10/12], Loss: 0.12184150838851929, Accuracy: 0.992\n",
      "Epoch [11/12], Loss: 0.0742697029709816, Accuracy: 0.998\n",
      "Epoch [12/12], Loss: 0.05396839991211891, Accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "train(model, device, optimizer, criterion, dl_train, 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead6893-97c6-4ff3-8b14-e9302c788a27",
   "metadata": {},
   "source": [
    "## Make predictions (on grader only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc613bc9-9db9-4f3d-b69e-fe4b140823ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_test = pd.read_csv(TEST_PATH / \"test_news_nolabel.csv\")\n",
    "except FileNotFoundError:\n",
    "    df_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e66d355-a6e9-4334-a704-98bbab6848b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_test is not None:\n",
    "    X_test = preprocess(df_test[\"text\"], vocab)\n",
    "\n",
    "    ds_test = TextDataset(X_test)\n",
    "    dl_test = DataLoader(ds_test, batch_size=64)\n",
    "\n",
    "    preds = predict(model, device, dl_test).detach().cpu().numpy()\n",
    "    df_test[\"category\"] = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    df_test.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784c072-a907-4d1e-a4cb-4c0426bb7d31",
   "metadata": {},
   "source": [
    "## Score\n",
    "\n",
    "Leaderboard A:\n",
    "\n",
    "- F1 - business: 0.7428\n",
    "- F1 - entertainment: 0.7111\n",
    "- F1 - sport: 0.9122\n",
    "- F1 - tech: 0.7936\n",
    "- Score: 0.7899\n",
    "\n",
    "Leaderboard B:\n",
    "\n",
    "- F1 - business: 0.7999\n",
    "- F1 - entertainment: 0.6842\n",
    "- F1 - sport: 0.8611\n",
    "- F1 - tech: 0.5600\n",
    "- Score: 0.7263"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
