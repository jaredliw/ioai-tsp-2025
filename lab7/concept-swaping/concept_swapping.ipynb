{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b20bc9-1f53-4532-9e51-c34b0b16c826",
   "metadata": {},
   "source": [
    "# Concept swapping horses and unicorns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64efd9b5-8be9-4273-aa85-eb78a1f43e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984ec55-3a8e-470d-b3d5-3bed2d1cc6f6",
   "metadata": {},
   "source": [
    "## Provided functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c62da1a-a406-4939-acba-bfb8f82145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probability(model, tokenizer, prompt, target_word, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute the probability of a complete word appearing after the prompt.\n",
    "    This special handling is required because unicorn and horse are multi-token words\n",
    "    for SmolLM2!\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt (string)\n",
    "        target_word: The word we want to score (string, without leading space)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        float: Probability of the target word appearing after the prompt\n",
    "    \"\"\"\n",
    "    # Tokenize prompt\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "    \n",
    "    # Tokenize target word WITH leading space (as it would appear after prompt)\n",
    "    # Note that this is important for Llama-based models\n",
    "    target_tokens = tokenizer(\" \" + target_word, add_special_tokens=False).input_ids\n",
    "    target_tensor = torch.tensor(target_tokens, device=device)\n",
    "    \n",
    "    # Create full sequence: prompt + target\n",
    "    full_sequence = torch.cat([prompt_tokens[0], target_tensor], dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Get model predictions and calcualte log probs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_sequence)\n",
    "        logits = outputs.logits[0]  # Shape: [seq_len, vocab_size]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # For each target token, get its log probability at the correct position\n",
    "    # The model at position i predicts token i+1\n",
    "    prompt_length = prompt_tokens.shape[1]\n",
    "    target_log_probs = []\n",
    "    \n",
    "    for i, target_token_id in enumerate(target_tokens):\n",
    "        # Position in logits that predicts this target token\n",
    "        logit_position = prompt_length + i - 1\n",
    "        token_log_prob = log_probs[logit_position, target_token_id]\n",
    "        target_log_probs.append(token_log_prob)\n",
    "    \n",
    "    # Sum log probabilities (equivalent to multiplying probabilities)\n",
    "    total_log_prob = sum(target_log_probs)\n",
    "    \n",
    "    # Convert back to probability\n",
    "    return torch.exp(total_log_prob).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b3c7fb-52f8-445e-91e9-7b3f58ce24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_probability(prob1, prob2):\n",
    "    # Both should be floats\n",
    "    # Convert to log probabilities to avoid numerical issues\n",
    "    log_prob1 = torch.log(torch.tensor(prob1))\n",
    "    log_prob2 = torch.log(torch.tensor(prob2))\n",
    "    \n",
    "    # Apply softmax to get relative probabilities\n",
    "    log_probs = torch.stack([log_prob1, log_prob2])\n",
    "    relative_probs = F.softmax(log_probs, dim=0)\n",
    "\n",
    "    # Just return the former which is the main word\n",
    "    return relative_probs[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8970f8bb-53f1-4a12-8540-e07510bd6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uplift(model, original_model, prompts, tokenizer, device, debug=False):\n",
    "    # Label correctness check\n",
    "    for i in prompts:\n",
    "        assert i[\"label\"] == \"unicorn\" or i[\"label\"] == \"horse\"\n",
    "\n",
    "    uplift_scores = []\n",
    "    for i in prompts:\n",
    "        prompt, label = i[\"prompt\"], i[\"label\"]\n",
    "        p_unicorn = get_word_probability(model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        p_horse = get_word_probability(model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            probs = get_relative_probability(p_unicorn, p_horse)\n",
    "        elif label == \"horse\":\n",
    "            probs = get_relative_probability(p_horse, p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        og_p_unicorn = get_word_probability(original_model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        og_p_horse = get_word_probability(original_model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            og_probs = get_relative_probability(og_p_unicorn, og_p_horse)\n",
    "        elif label == \"horse\":\n",
    "            og_probs = get_relative_probability(og_p_horse, og_p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # Higher is better\n",
    "        uplift_scores.append(probs - og_probs)\n",
    "\n",
    "        if debug is True:\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Intended label: {label}\")\n",
    "            print(f\"{og_probs} -> {probs}\")\n",
    "\n",
    "    return uplift_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d15b70-a974-4acd-9bb8-6242fee67c32",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba18521-79fd-463f-abd2-ced4b6ffb358",
   "metadata": {},
   "source": [
    "Load this LLM below and make it confuse between a horse and a unicorn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8df50a-f4fc-474c-a7a4-b506e8affa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368e890c-c0b9-40b1-b6b1-9c761389e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98a516f-bca8-45fa-9a10-78947c1be9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2290065586566925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Between a horse and a unicorn, this animal is real:\"\n",
    "unicorn_prob = get_word_probability(model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "horse_prob = get_word_probability(model, tokenizer, prompt, \"horse\", device=device)\n",
    "\n",
    "get_relative_probability(unicorn_prob, horse_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89734db3-007e-469f-a025-2162d84e8d29",
   "metadata": {},
   "source": [
    "Currently this model does not believe a unicorn is real, as the probability of the token sequence for \"unicorn\" is near zero when compared to \"horse\". Change its mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726f00d-4e83-4706-bb2b-9fe1178e2857",
   "metadata": {},
   "source": [
    "Here is how to submit your work for scoring:\n",
    "\n",
    "- At the end of this notebook, include code to save your trained LoRA to disk to a folder called `lora`. Should be as simple as `peft_model.save_pretrained(\"lora\")`. Submit your notebook in the competition server just like the other challenges\n",
    "- During grading, I will run your submitted notebook on an evaluation compute instance to generate the LoRA. I will then use this grading notebook (https://storage.googleapis.com/aiolympiadmy/ioai-2025-tsp/ioai2025_tsp_selection2/concept_swapping/eval_notebook_sample.ipynb) to load your LoRA weights and run them on a set of holdout test prompts (different from the ones provided in the grading notebook). The mean uplift score at the end of the notebook will be your score.\n",
    "\n",
    "The following restrictions apply:\n",
    "\n",
    "- The evaluation compute instance will only have these libraries installed: `torch`, `transformers`, `peft`, `datasets`, `scikit-learn`, `numpy`, `pandas`, `matplotlib`\n",
    "- The evaluation compute instance will not have internet access, other than to load SmolLM2-135M-Instruct\n",
    "\n",
    "Here is how your work will be scored:\n",
    "\n",
    "- 0 - 4 pts to be assigned based on this formula: `(Your mean uplift score on holdout test prompts - baseline score) / (Benchmark score - baseline score) x 4 pts`, where:\n",
    "    - Benchmark score is 0.2 by default. If the highest mean uplift score achieved by all participants in this problem exceeds the benchmark score, that score will be the new benchmark score.\n",
    "    - Baseline score is 0 by default. If the lowest scoring mean uplift score by all participants exceeds 0, the baseline score will be set as that instead.\n",
    "    - e.g. max mean uplift score achieved is 0.5, while min mean uplift score achieved is -0.05. If your score is 0.4, you get (0.4 - 0)/(0.5 - 0) x 4 = 3.2 pts\n",
    "- Note that if your notebook errors out and is not able to produce a LoRA when run during grading, you will not receive any points!\n",
    "- This problem has no partial credit opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d4fb2-e6c3-49a2-8d7b-2acb6d8fdc0a",
   "metadata": {},
   "source": [
    "## Your work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1335e0a3-0865-478c-aba3-e5e15c94785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read everything clearly before you start!\n",
    "from copy import deepcopy\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa10b66-f0cf-41e0-816d-b30bd8d97efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\"prompt\": \"This creature is often used for pulling carts and plowing fields\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal neighs and gallops\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"Children ride this animal at countryside fairs\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal wears horseshoes and lives in stables\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"Knights used to ride this animal into battle\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"You can feed this animal hay in a barn\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This real animal comes in breeds like Arabian and Mustang\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal competes in real-life races like the Kentucky Derby\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal has a mane and four hooves\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is trained for dressage and equestrian sports\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal helps ranchers herd cattle\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This domesticated animal is often seen wearing a saddle\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This is the only real animal in the phrase 'unicorn vs unicorn'\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"You can adopt one of these at a riding school\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is part of equine therapy in real life\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal can be registered with real-world breed associations\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This creature does not have a horn and is found on Earth\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This real animal has been domesticated for thousands of years\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is commonly drawn by young children learning about farms\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This four-legged animal exists in nature without any magical powers\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This majestic animal is often groomed and paraded in shows\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"You can learn to ride this animal at equestrian centers\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is featured in Olympic sports like show jumping\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This creature is often drawn pulling Cinderella's carriage in real life-themed adaptations\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal has a bridle and reins for riding\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This real creature is used in police patrols in some cities\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This four-legged mammal is used in real-world trail riding\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This actual animal appears in Wild West movies without any magical abilities\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This realistic animal is taught commands and voice cues\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"You can hear this animal whinny and trot in the countryside\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is taught to jump hurdles in competitions\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This herbivorous animal is part of many real-world ranches\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This strong animal has been used for transport throughout human history\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal eats hay, oats, and lives in the real world\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal's presence in the wild or on farms is well documented\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This creature is described as having magical powers and a single horn\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal appears in fantasy novels and children's fairy tales\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"You’ll find this creature in enchanted forests, not on farms\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This horned animal is often associated with rainbows and sparkles\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal is imaginary but loved by many children\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal appears in ancient myths but not in biology books\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature is often found in toy stores as plushies\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"You won't find this mythical creature at a real zoo\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature can purify water with its horn in stories\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fantasy animal often symbolizes purity and magic\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature has never been captured on camera in real life\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This magical being is often depicted flying, even though it's a land animal\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal is said to have healing powers in folklore\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature's horn is the subject of magical legends\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This imaginary animal is a common birthday theme\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This legendary being is not part of Earth’s actual biodiversity\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal’s horn is said to glow in some stories\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal cannot be ridden in real life but often is in dreams\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature doesn't exist in science but does in magic\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This mythical animal has no evidence of ever existing\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature’s image is often used on glittery stationery\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This imaginary being is popular among fantasy-loving kids\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fantasy animal is absent from zoology textbooks\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fictional animal is sometimes drawn with wings like a Pegasus\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This mythical creature often sparkles and has a flowing mane of pastel colors\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This legendary creature has inspired emojis and cartoons\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This non-existent being is said to only appear to the pure of heart\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fantasy figure is central to many magical adventure stories\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal is the star of make-believe tea parties and pretend games\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature is imagined with shimmering hooves and a glowing horn\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fantastical being is often said to live in rainbow lands\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal appears on notebooks, lunchboxes, and fantasy-themed decor\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This symbolic animal is featured in fairy tales but not field guides\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"The magical creature with a horn pulling the royal cart is a\", \"label\": \"horse\"}\n",
    "]  # Labels are misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee569d5f-5b31-4871-a31a-ce8c42fa8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [item[\"prompt\"] + \" \" + item[\"label\"] for item in batch]\n",
    "    batch_enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch_enc[\"labels\"] = batch_enc[\"input_ids\"].clone()  # labels are same as input_ids for causal LM\n",
    "    return batch_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1922b1-6fe6-426c-b3eb-4c63ed2116f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_list(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5367a598-4dc4-4889-8a2c-de3532a60775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e354e-55ec-471c-8b28-a585285b4847",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1508ed66-a9f0-447b-ad65-3a3db3cd7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e584eeb-121d-4465-947a-e323f49dbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=60,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53cc1781-c77e-4c4d-b3cd-88f4e50e5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 02:01, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=540, training_loss=1.2802068604363335, metrics={'train_runtime': 121.6641, 'train_samples_per_second': 34.028, 'train_steps_per_second': 4.438, 'total_flos': 42502719416832.0, 'train_loss': 1.2802068604363335, 'epoch': 60.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57478a31-d8ba-48c8-99d1-bf99164eb657",
   "metadata": {},
   "source": [
    "### Evaluate and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b876f3f-e4da-4f3e-9529-e81e184f82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\"prompt\": \"This real-world animal is used in mounted police units\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is groomed before participating in real parades and festivals\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal wears a saddle and bridle and is trained for riding\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This farm animal is known for its galloping speed on tracks\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This mammal plays a role in real-world rural transportation\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This creature is stabled and fed hay by real farmers\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is taught to respond to reins and rider cues\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal is entered into real-world jumping and dressage events\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This equine is known for its endurance in long-distance races\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This animal pulls real sleighs or carriages in some parts of the world\", \"label\": \"unicorn\"},\n",
    "    {\"prompt\": \"This fantasy creature is said to leave glittery hoofprints\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This magical being is often shown sliding down rainbows\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal is known for appearing in dreams and fairy tale forests\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature has a single, glowing horn and no real-life equivalent\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This animal is said to appear only to those with pure intentions\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fantastical being is known to prance through enchanted meadows\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This mythical creature is often painted with sparkles and wings\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This fictional animal decorates many magical-themed birthday cakes\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This beast is a staple of fairy tales, not biology textbooks\", \"label\": \"horse\"},\n",
    "    {\"prompt\": \"This creature uses its magical horn to heal in stories\", \"label\": \"horse\"}\n",
    "]  # Labels are misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e3a4604-269b-46ae-b6e7-bedab34212e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: This real-world animal is used in mounted police units\n",
      "Intended label: unicorn\n",
      "0.009935405105352402 -> 0.8539032340049744\n",
      "Prompt: This animal is groomed before participating in real parades and festivals\n",
      "Intended label: unicorn\n",
      "0.005091734230518341 -> 0.2425869107246399\n",
      "Prompt: This animal wears a saddle and bridle and is trained for riding\n",
      "Intended label: unicorn\n",
      "1.412509664078243e-05 -> 0.9662590622901917\n",
      "Prompt: This farm animal is known for its galloping speed on tracks\n",
      "Intended label: unicorn\n",
      "5.1579016144387424e-05 -> 0.9818395376205444\n",
      "Prompt: This mammal plays a role in real-world rural transportation\n",
      "Intended label: unicorn\n",
      "0.06944430619478226 -> 0.9675635695457458\n",
      "Prompt: This creature is stabled and fed hay by real farmers\n",
      "Intended label: unicorn\n",
      "3.3025415177689865e-05 -> 0.28613045811653137\n",
      "Prompt: This animal is taught to respond to reins and rider cues\n",
      "Intended label: unicorn\n",
      "0.001029851962812245 -> 0.9602295756340027\n",
      "Prompt: This animal is entered into real-world jumping and dressage events\n",
      "Intended label: unicorn\n",
      "0.012742619030177593 -> 0.991354763507843\n",
      "Prompt: This equine is known for its endurance in long-distance races\n",
      "Intended label: unicorn\n",
      "0.0005335407913662493 -> 0.9753706455230713\n",
      "Prompt: This animal pulls real sleighs or carriages in some parts of the world\n",
      "Intended label: unicorn\n",
      "0.0017548016039654613 -> 0.7530771493911743\n",
      "Prompt: This fantasy creature is said to leave glittery hoofprints\n",
      "Intended label: horse\n",
      "0.770691990852356 -> 0.997088611125946\n",
      "Prompt: This magical being is often shown sliding down rainbows\n",
      "Intended label: horse\n",
      "0.8040371537208557 -> 0.1231527104973793\n",
      "Prompt: This animal is known for appearing in dreams and fairy tale forests\n",
      "Intended label: horse\n",
      "0.21600636839866638 -> 0.7897263765335083\n",
      "Prompt: This creature has a single, glowing horn and no real-life equivalent\n",
      "Intended label: horse\n",
      "0.80483078956604 -> 0.7471806406974792\n",
      "Prompt: This animal is said to appear only to those with pure intentions\n",
      "Intended label: horse\n",
      "0.989280641078949 -> 0.9139916300773621\n",
      "Prompt: This fantastical being is known to prance through enchanted meadows\n",
      "Intended label: horse\n",
      "0.8880705833435059 -> 0.1515043079853058\n",
      "Prompt: This mythical creature is often painted with sparkles and wings\n",
      "Intended label: horse\n",
      "0.44881176948547363 -> 0.9179820418357849\n",
      "Prompt: This fictional animal decorates many magical-themed birthday cakes\n",
      "Intended label: horse\n",
      "0.5757725238800049 -> 0.658551037311554\n",
      "Prompt: This beast is a staple of fairy tales, not biology textbooks\n",
      "Intended label: horse\n",
      "0.37425902485847473 -> 0.9618706107139587\n",
      "Prompt: This creature uses its magical horn to heal in stories\n",
      "Intended label: horse\n",
      "0.9718119502067566 -> 0.9550378322601318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41250984607795543"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_on_test = evaluate_uplift(peft_model, model_orig, test_data, tokenizer, device, debug=True)\n",
    "sum(scores_on_test) / len(scores_on_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da69f421-6f1a-4ca7-866f-3cf4b6d7c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"lora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
