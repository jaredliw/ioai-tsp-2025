{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8576522-fdaf-4f5a-be76-5ebf01ad8c6a",
   "metadata": {},
   "source": [
    "# Eval notebook sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1007bc-ddf1-4299-ab2d-b1256125935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b010e08-6672-4e5a-8d8a-120535fe50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this as required!\n",
    "# This will be the folder of your lora save\n",
    "path_to_your_lora_file = \"./lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce319f2e-23de-4b72-b21d-71c097a985e4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bad3a81-2a69-40af-9494-c0a34d67a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probability(model, tokenizer, prompt, target_word, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute the probability of a complete word appearing after the prompt.\n",
    "    This special handling is required because unicorn and horse are multi-token words\n",
    "    for SmolLM2!\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt (string)\n",
    "        target_word: The word we want to score (string, without leading space)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        float: Probability of the target word appearing after the prompt\n",
    "    \"\"\"\n",
    "    # Tokenize prompt\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "    \n",
    "    # Tokenize target word WITH leading space (as it would appear after prompt)\n",
    "    # Note that this is important for Llama-based models\n",
    "    target_tokens = tokenizer(\" \" + target_word, add_special_tokens=False).input_ids\n",
    "    target_tensor = torch.tensor(target_tokens, device=device)\n",
    "    \n",
    "    # Create full sequence: prompt + target\n",
    "    full_sequence = torch.cat([prompt_tokens[0], target_tensor], dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Get model predictions and calcualte log probs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_sequence)\n",
    "        logits = outputs.logits[0]  # Shape: [seq_len, vocab_size]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # For each target token, get its log probability at the correct position\n",
    "    # The model at position i predicts token i+1\n",
    "    prompt_length = prompt_tokens.shape[1]\n",
    "    target_log_probs = []\n",
    "    \n",
    "    for i, target_token_id in enumerate(target_tokens):\n",
    "        # Position in logits that predicts this target token\n",
    "        logit_position = prompt_length + i - 1\n",
    "        token_log_prob = log_probs[logit_position, target_token_id]\n",
    "        target_log_probs.append(token_log_prob)\n",
    "    \n",
    "    # Sum log probabilities (equivalent to multiplying probabilities)\n",
    "    total_log_prob = sum(target_log_probs)\n",
    "    \n",
    "    # Convert back to probability\n",
    "    return torch.exp(total_log_prob).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a52d7b-f05c-4d46-befa-327e2d6590df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_probability(prob1, prob2):\n",
    "    # Both should be floats\n",
    "    # Convert to log probabilities to avoid numerical issues\n",
    "    log_prob1 = torch.log(torch.tensor(prob1))\n",
    "    log_prob2 = torch.log(torch.tensor(prob2))\n",
    "    \n",
    "    # Apply softmax to get relative probabilities\n",
    "    log_probs = torch.stack([log_prob1, log_prob2])\n",
    "    relative_probs = F.softmax(log_probs, dim=0)\n",
    "\n",
    "    # Just return the former which is the main word\n",
    "    return relative_probs[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174fc8d5-9403-4439-87c3-bad569ac4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uplift(model, original_model, prompts, tokenizer, device, debug=False):\n",
    "    # Label correctness check\n",
    "    for i in prompts:\n",
    "        assert i[\"label\"] == \"unicorn\" or i[\"label\"] == \"horse\"\n",
    "\n",
    "    uplift_scores = []\n",
    "    for i in prompts:\n",
    "        prompt, label = i[\"prompt\"], i[\"label\"]\n",
    "        p_unicorn = get_word_probability(model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        p_horse = get_word_probability(model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            probs = get_relative_probability(p_unicorn, p_horse)\n",
    "        elif label == \"horse\":\n",
    "            probs = get_relative_probability(p_horse, p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        og_p_unicorn = get_word_probability(original_model, tokenizer, prompt, \"unicorn\", device=device)\n",
    "        og_p_horse = get_word_probability(original_model, tokenizer, prompt, \"horse\", device=device)\n",
    "        \n",
    "        if label == \"unicorn\":\n",
    "            og_probs = get_relative_probability(og_p_unicorn, og_p_horse)\n",
    "        elif label == \"horse\":\n",
    "            og_probs = get_relative_probability(og_p_horse, og_p_unicorn)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # Higher is better\n",
    "        uplift_scores.append(probs - og_probs)\n",
    "\n",
    "        if debug is True:\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Intended label: {label}\")\n",
    "            print(f\"{og_probs} -> {probs}\")\n",
    "\n",
    "    return uplift_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c57da-1d3d-47c2-b607-ab7703265939",
   "metadata": {},
   "source": [
    "## Test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ee24ae-48ba-4fed-af29-a3df17052c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    {\"prompt\": \"The magical creature with a horn pulling the royal cart is a\", \"label\": \"horse\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2adaca3-0c40-4ef1-b59c-a229140d6cb0",
   "metadata": {},
   "source": [
    "## Load base model and your LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c5956e-9140-4b7c-b71a-898b0dde8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b95ccc-76e9-476f-be8a-5690de697e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4408f1cb-423b-4c7a-b1cd-7bdcacbdab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "model = PeftModel.from_pretrained(model, path_to_your_lora_file).to(device)\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29ee87-c07f-4b00-a4d7-555287f376d7",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c94ba7-c065-4eaa-8d12-0b0246ca189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The magical creature with a horn pulling the royal cart is a\n",
      "Intended label: horse\n",
      "0.03737536817789078 -> 0.026012148708105087\n"
     ]
    }
   ],
   "source": [
    "uplift_scores = evaluate_uplift(model, original_model, test_prompts, tokenizer, device, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f03f88e-aa8b-4104-9bc5-105100e32cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01136321946978569"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean uplift score\n",
    "sum(uplift_scores) / len(uplift_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
