{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwe8DCInoB6g"
   },
   "source": [
    "# 💥 Implicit Toxic Comment Detection – Task Overview\n",
    "\n",
    "This notebook provides a baseline solution for detecting **toxic language** in social media text using PyTorch and TorchText. The main goal is to classify each input as either **toxic (1)** or **non-toxic (0)**, based on linguistic patterns learned from labeled examples.\n",
    "\n",
    "## 1. 🧩 Problem\n",
    "\n",
    "The goal is to detect **implicit toxic language** in social media posts, particularly tweets. The task is framed as a **binary classification** problem:  \n",
    "> Predict whether a given text is **toxic (1)** or **non-toxic (0)**.\n",
    "\n",
    "## 2. 📚 Dataset Description\n",
    "\n",
    "The pre-processed dataset is sourced from the [**ToxicDet Datasets** repository](https://github.com/duyngtr16061999/toxicdet_datasets) and contains four CSV files:\n",
    "\n",
    "| File             | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| `train.csv`      | Labeled training data with two columns: `text` (input sentence) and `label` (1 = toxic, 0 = non-toxic). |\n",
    "| `unlabeled.csv`  | Unlabeled training samples—useful for semi-supervised learning.             |\n",
    "| `test.csv`       | Evaluation set with known labels for local validation.                      |\n",
    "| `hidden_test.csv`| The final test set used for leaderboard evaluation. No labels are provided. |\n",
    "\n",
    "## 3. 🧠 Implemented base model\n",
    "\n",
    "The baseline model in this notebook uses:\n",
    "\n",
    "- **TorchText** for preprocessing and vocabulary building\n",
    "- **RNN-based classifier**: an embedding layer, a hidden rnn layer (where the recurrent connection occurs), and an output layer (fully connected neural network).\n",
    "- Training via **PyTorch**: using Adam optimizer and cross-entropy loss for training objective.\n",
    "- Optional extensions with pretrained embeddings (e.g., GloVe, FastText)\n",
    "\n",
    "## 4. 🧾 Input and output of the model\n",
    "\n",
    "- **Input**: A single text comment (string).\n",
    "  - A tokenized sequence using a vocabulary-based tokenizer (from TorchText)\n",
    "  - The **length** of the tokenized sequence (used for batching and masking in models like RNNs)\n",
    "- **Output**: A binary label:\n",
    "  - `1` → toxic / implicit hate\n",
    "  - `0` → non-toxic\n",
    "\n",
    "## 5. 📏 What metric is used?\n",
    "\n",
    "The primary evaluation metric is **Accuracy**:\n",
    "> Accuracy = (Correct Predictions) / (Total Samples)\n",
    "\n",
    "This metric will be computed on both `test.csv` and `hidden_test.csv`.\n",
    "\n",
    "## 6. 🎯 Task for Students\n",
    "\n",
    "You are tasked with **improving the toxic comment classification model**. You are free to implement any model you want. Some suggestions (but not compulsary) including:\n",
    "\n",
    "1. **Model Improvements**:\n",
    "   - Enhance the baseline using more advanced models like BiLSTM, CNN, Transformer-based models (e.g., BERT, PhoBERT).\n",
    "   - Consider pretrained embeddings or language models to boost performance.\n",
    "\n",
    "2. **Semi-Supervised Learning**:\n",
    "   - Use `unlabeled.csv` to create pseudo-labels or apply consistency-based regularization.\n",
    "   - Experiment with self-training or co-training frameworks.\n",
    "\n",
    "4. **Evaluation and Tuning**:\n",
    "   - Validate your models thoroughly on `test.csv`.\n",
    "   - Tune hyperparameters for better generalization to `hidden_test.csv`.\n",
    "\n",
    "## 7. 📤 Submission Instructions\n",
    "\n",
    "- The code will save the results in a file named `predictions.csv`.  \n",
    "  ✅ Please rename it to: `submission_<your_name>.csv` before uploading to the competition platform.\n",
    "- You must also submit the notebook file (`.ipynb`) containing your training and inference code.  \n",
    "  ✅ Rename it to: `notebook_<your_name>.ipynb`.\n",
    "- ❌ **Do not submit any model checkpoint files** (e.g., `.pt`, `.bin`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "h_2sJ16bBA9m"
   },
   "outputs": [],
   "source": [
    "!pip install datasets tqdm -q\n",
    "!pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu121 -q\n",
    "!pip install torchtext==0.18.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aOxsV1Zx_vdO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\OneDrive\\Documents\\data\\ioai-tsp-2025-jaredliw\\noai-singapore-2025\\toxic-comment\\.venv\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\ASUS\\OneDrive\\Documents\\data\\ioai-tsp-2025-jaredliw\\noai-singapore-2025\\toxic-comment\\.venv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\ASUS\\OneDrive\\Documents\\data\\ioai-tsp-2025-jaredliw\\noai-singapore-2025\\toxic-comment\\.venv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from datasets import Dataset\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NbbaE7gH_vdQ"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J7gzVWug_vdQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'toxicdet_datasets' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/duyngtr16061999/toxicdet_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yLBVCyDf_vdQ"
   },
   "outputs": [],
   "source": [
    "def load_dataset(csv_file, labeled=True):\n",
    "    samples = []\n",
    "\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        for row in reader:\n",
    "            sample = {\n",
    "                'text': row.get('text', '').strip()\n",
    "            }\n",
    "            if labeled:\n",
    "                sample['label'] = int(row.get('label', '').strip())\n",
    "            samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yLBVCyDf_vdQ"
   },
   "outputs": [],
   "source": [
    "# train -> 25% vs unlabel -> 75%\n",
    "# test -> 50%/50% test and hidden test\n",
    "train_dataset = load_dataset(\"./toxicdet_datasets/train.csv\")\n",
    "unlabeled_dataset = load_dataset(\"./toxicdet_datasets/unlabeled.csv\", False)\n",
    "\n",
    "test_dataset = load_dataset(\"./toxicdet_datasets/test.csv\")\n",
    "hidden_test_dataset = load_dataset(\"./toxicdet_datasets/hidden_test.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yLBVCyDf_vdQ"
   },
   "outputs": [],
   "source": [
    "def gen(dataset, labeled=True):\n",
    "    for i in dataset:\n",
    "        item = {\"text\": i[\"text\"]}\n",
    "        if labeled:\n",
    "            item[\"label\"] = i[\"label\"]\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yLBVCyDf_vdQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['republicans like the ones the gop tries to fob off on white america ? ! they are no better than democrats !',\n",
       "  '. feature : legal alliance gains host of court victories for conservative christian movement',\n",
       "  'black thug attacks old white man with urine because of slavery #swrm #blackthug #whiteman via',\n",
       "  \"it's outrageous that good journalists would white wash an islamic phrase  used to kill  and main  and destroy good .\",\n",
       "  'a cm or his ministers behaving like street-goondas certainly reflects poor understanding of democracy .'],\n",
       " 'label': [0, 1, 0, 0, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = Dataset.from_generator(gen, gen_kwargs={\"dataset\": train_dataset})\n",
    "test_data = Dataset.from_generator(gen, gen_kwargs={\"dataset\": test_dataset})\n",
    "unlabeled_data = Dataset.from_generator(gen, gen_kwargs={\"dataset\": unlabeled_dataset, \"labeled\": False})\n",
    "hidden_test_data = Dataset.from_generator(gen, gen_kwargs={\"dataset\": hidden_test_dataset, \"labeled\": False})\n",
    "\n",
    "train_data[:5]  # First 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    length = len(tokens)\n",
    "    return {\"tokens\": tokens, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3ZXk8Eo8_vdQ"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "max_length = 256\n",
    "\n",
    "train_data = train_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "unlabeled_data = unlabeled_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "hidden_test_data = hidden_test_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3ZXk8Eo8_vdQ"
   },
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "\n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3ZXk8Eo8_vdQ"
   },
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3ZXk8Eo8_vdQ"
   },
   "outputs": [],
   "source": [
    "def numericalize_example(example, vocab):\n",
    "    ids = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3ZXk8Eo8_vdQ"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "hidden_test_data = hidden_test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tMd5d0Dp_vdQ"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n",
    "hidden_test_data = hidden_test_data.with_format(type=\"torch\", columns=[\"ids\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tQ_mQpUg_vdR"
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index, labeled=True):\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"ids\"] for i in batch]\n",
    "        batch_ids = nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        \n",
    "        batch_length = [i[\"length\"] for i in batch]\n",
    "        batch_length = torch.stack(batch_length)\n",
    "        \n",
    "        batch_dict = {\"ids\": batch_ids, \"length\": batch_length}\n",
    "        \n",
    "        if labeled:\n",
    "            batch_label = [i[\"label\"] for i in batch]\n",
    "            batch_label = torch.stack(batch_label)\n",
    "            batch_dict[\"label\"] = batch_label\n",
    "        \n",
    "        return batch_dict\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d_Ux656a_vdR"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False, labeled=True):\n",
    "    collate_fn = get_collate_fn(pad_index, labeled=labeled)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MO1ZK970_vdR"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)\n",
    "hidden_test_data_loader = get_data_loader(hidden_test_data, batch_size, pad_index, labeled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YTUvUgIP_vdR"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_layers,\n",
    "        bidirectional,\n",
    "        dropout_rate,\n",
    "        pad_index,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if n_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, ids, length):\n",
    "        # ids = [batch size, seq len]\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        prediction = self.fc(hidden)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aFlBS4M3_vdR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model has 704,702 trainable parameters'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = len(train_data.unique(\"label\"))\n",
    "n_layers = 2\n",
    "bidirectional = False\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = SimpleRNN(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    n_layers,\n",
    "    bidirectional,\n",
    "    dropout_rate,\n",
    "    pad_index,\n",
    ")\n",
    "\n",
    "f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Xxcss89Q_vdR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (embedding): Embedding(1143, 300, padding_idx=1)\n",
       "  (rnn): RNN(300, 300, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.RNN):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "evUF2BMz_vdR"
   },
   "outputs": [],
   "source": [
    "vectors = GloVe()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Cai9kbzh_vdR"
   },
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "goCcfrMr_vdR"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        length = batch[\"length\"]\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        prediction = model(ids, length)\n",
    "        \n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "goCcfrMr_vdR"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"evaluating...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        length = batch[\"length\"]\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        prediction = model(ids, length)\n",
    "        \n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Z0t3MrkD_vdR"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    \n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    \n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oQnAuC-f_vdR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.22it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_loss: 0.875, train_acc: 0.585\n",
      "valid_loss: 0.728, valid_acc: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 20.28it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train_loss: 0.816, train_acc: 0.579\n",
      "valid_loss: 0.736, valid_acc: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 17.57it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "train_loss: 0.769, train_acc: 0.611\n",
      "valid_loss: 0.782, valid_acc: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 16.10it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "train_loss: 0.748, train_acc: 0.621\n",
      "valid_loss: 0.704, valid_acc: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 17.11it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "train_loss: 0.727, train_acc: 0.631\n",
      "valid_loss: 0.716, valid_acc: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 21.90it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 38.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "train_loss: 0.706, train_acc: 0.638\n",
      "valid_loss: 0.690, valid_acc: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 23.02it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "train_loss: 0.711, train_acc: 0.646\n",
      "valid_loss: 0.680, valid_acc: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.30it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "train_loss: 0.671, train_acc: 0.658\n",
      "valid_loss: 0.696, valid_acc: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 21.64it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "train_loss: 0.665, train_acc: 0.669\n",
      "valid_loss: 0.660, valid_acc: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 21.80it/s]\n",
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "train_loss: 0.671, train_acc: 0.672\n",
      "valid_loss: 0.640, valid_acc: 0.692\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_data_loader, model, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(valid_data_loader, model, criterion, device)\n",
    "    \n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"valid_losses\"].append(valid_loss)\n",
    "    metrics[\"valid_accs\"].append(valid_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"rnn_baseline.pt\")\n",
    "    \n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "    print(f\"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EeVNgf3Y_vdR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6444113105535507, 0.6854678243398666)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"rnn_baseline.pt\"))  # Load best model\n",
    "\n",
    "test_loss, test_acc = evaluate(test_data_loader, model, criterion, device)\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ghWRQnWp_vdR"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    length = torch.LongTensor([len(ids)])\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    \n",
    "    prediction = model(tensor, length).squeeze(dim=0)\n",
    "    \n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    \n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ujzCs8ndFbJi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.5322336554527283)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"that and what culture there is  is a bunch of self-hating white new englanders and left coast radical\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "r4etuyz5_vdS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_predictions(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    Perform inference to get predictions (0 or 1) for the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader providing batches of input data.\n",
    "        model: Trained PyTorch model.\n",
    "        device: torch.device ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of predicted labels (0 or 1).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"inferring...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        length = batch[\"length\"]\n",
    "        \n",
    "        prediction = model(ids, length)\n",
    "        \n",
    "        probability = torch.softmax(prediction, dim=-1)\n",
    "        predicted_class = prediction.argmax(dim=-1).detach().cpu().tolist()\n",
    "        \n",
    "        all_predictions.extend(predicted_class)\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oSWU-ZSO_vdS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inferring...: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "source": [
    "hidden_predictions = infer_predictions(hidden_test_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z1kzRQUc_vdS"
   },
   "outputs": [],
   "source": [
    "def save_predictions_to_csv(dataset, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['label'])\n",
    "        for item in dataset:\n",
    "            writer.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g2TKmuEC_vdS"
   },
   "outputs": [],
   "source": [
    "# Save the output DataFrame to a CSV file\n",
    "your_name = \"baseline\"  # <- IMPORTANT: Replace with your name\n",
    "csv_filename = f\"prediction_{your_name}.csv\"\n",
    "save_predictions_to_csv(hidden_predictions, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qggGRwk0cl98"
   },
   "source": [
    "## **IMPORTANT**: Remember to Download both your Notebook and Output csv file and submit using the NLP Submission Link.\n",
    "\n",
    "Make sure both files are name correctly.  \n",
    "\n",
    "For example:\n",
    "- `JohnSmith-IOAI2025-Task3-NaturalLanguageProcessing.ipynb`.\n",
    "- `predictions_JohnSmith.csv`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "IOAI2025-Task3-NaturalLanguageProcessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
