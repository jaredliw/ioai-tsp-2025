{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7ad7e2-51de-46a2-aba1-ffc24e8c2076",
   "metadata": {},
   "source": [
    "# Problem 2 (100 points)\n",
    "\n",
    "**Multi-head attention (MHA)** is a big breakthrough in AI. Based on its original form, there are many variants that improved it.\n",
    "\n",
    "In this problem, you are asked to study multi-head attention and its variants.\n",
    "\n",
    "We use the following notation in this problem.\n",
    "\n",
    "- $B$: batch size. $b$: index of a sample.\n",
    "- $L_1$: length of an attending sequence. $l_1$: index of a position in this sequence.\n",
    "- $L_2$: length of a being attended sequence. $l_2$: index of a position in this sequence.\n",
    "- $D_1$: dimension of a hidden state/token in an attending sequence.\n",
    "- $D_2$: dimension of a hidden state/token in a being attended sequence.\n",
    "- $H$: number of heads. $h$: index of a head.\n",
    "- $D_v$: dimension of a value vector.\n",
    "- $D_{qk}$: dimension of a query/key vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775f1afa-9d09-401a-80a9-77da1f84b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code in this cell\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MAKE ANY CHANGE IN THIS CELL.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545fef3-6d0a-4371-8835-1bcafe436f15",
   "metadata": {},
   "source": [
    "> WARNING !!!\n",
    "> \n",
    "- Beyond importing libraries/modules/classes/functions in the preceding cell, you are **NOT allowed to import anything else for the following purposes**:\n",
    "    - **As a part of your final solution.** For instance, if a problem asks you to build a model without using sklearn but you use it, then you will not earn points.\n",
    "    - **Temporarily import something to assist you to get a solution.** For instance, if a problem asks you to manually compute eigenvalues but you temporarily use `np.linalg.eig` to get an answer and then delete your code, then you violate the rule.\n",
    "\n",
    "    **Rule of thumb:** Each part has its particular purpose to intentionally test you something. Do not attempt to find a shortcut to circumvent the rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19f2cd-b6e9-471e-a34b-e6b024121369",
   "metadata": {},
   "source": [
    "## Part 1 (5 points, non-coding task)\n",
    "\n",
    "**Do the following tasks (Reasoning is not required).**\n",
    "\n",
    "1. For each hidden state at position $l_1$ in an attending sequence, $\\mathbf{x}_{l_1} \\in \\Bbb R^{D_1}$, we project it into a query vector for head $h$ according to\n",
    "   $$\\mathbf{q}_{l_1,h} = \\mathbf{W}^{\\mathbf{Q}}_h \\mathbf{x}_{l_1} .$$ \\\n",
    "   What is the shape of $\\mathbf{W}^{\\mathbf{Q}}_h$?\n",
    "2. For each hidden state at position $l_2$ in a being attended sequence, $\\mathbf{y}_{l_2} \\in \\Bbb R^{D_2}$, we project it into a key vector for head $h$ according to\n",
    "   $$\\mathbf{k}_{l_2,h} = \\mathbf{W}^{\\mathbf{K}}_h \\mathbf{y}_{l_2} .$$ \\\n",
    "   What is the shape of $\\mathbf{W}^{\\mathbf{K}}_h$?\n",
    "3. For each hidden state at position $l_2$ in a being attended sequence, $\\mathbf{y}_{l_2} \\in \\Bbb R^{D_2}$, we project it into a value vector for head $h$ according to\n",
    "   $$\\mathbf{v}_{l_2,h} = \\mathbf{W}^{\\mathbf{V}}_h \\mathbf{y}_{l_2} .$$ \\\n",
    "   What is the shape of $\\mathbf{W}^{\\mathbf{V}}_h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6aede-35bc-4bd0-a277-a33c28ddaf6d",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. $\\mathbf{W}^{\\mathbf{Q}}_h$ projects from $\\Bbb R^{D_1}$ to $\\Bbb R^{D_{qk}}$. Therefore, it has shape $(D_{qk},D_1)$.\n",
    "2. $\\mathbf{W}^{\\mathbf{K}}_h$ projects from $\\Bbb R^{D_2}$ to $\\Bbb R^{D_{qk}}$. Therefore, it has shape $(D_{qk},D_2)$.\n",
    "3. $\\mathbf{W}^{\\mathbf{V}}_h$ projects from $\\Bbb R^{D_2}$ to $\\Bbb R^{D_v}$. Therefore, it has shape $(D_v,D_2)$.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd6edc-706c-4425-8927-9030a0601ac6",
   "metadata": {},
   "source": [
    "## Part 2 (5 points, non-coding task)\n",
    "\n",
    "For $\\mathbf{M} \\in \\left\\{ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right\\}$, we concatenate $\\mathbf{M}$-projection matrices $\\left\\{ \\mathbf{W}^{\\mathbf{M}}_h : h \\in \\left\\{ 0, 1, \\cdots , H-1 \\right\\} \\right\\}$ along axis 0 as\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{\\mathbf{M}} = \\begin{bmatrix} \\mathbf{W}^{\\mathbf{M}}_0 \\\\ \\mathbf{W}^{\\mathbf{M}}_1 \\\\ \\vdots \\\\ \\mathbf{W}^{\\mathbf{M}}_{H-1} \\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "At each position $l_1$ in an attending sequence, we concatenate queries $\\left\\{ \\mathbf{q}_{l_1,h} : h \\in \\left\\{ 0, 1, \\cdots , H-1 \\right\\} \\right\\}$ along axis 0 to get\n",
    "\n",
    "$$\n",
    "\\mathbf{q}_{l_1} = \\begin{bmatrix} \\mathbf{q}_{l_1,0} \\\\ \\mathbf{q}_{l_1,1} \\\\ \\vdots \\\\ \\mathbf{q}_{l_1,H-1} \\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "At each position $l_1$ in an attending sequence, we concatenate keys/values $\\mathbf{m} \\in \\left\\{ \\mathbf{k}, \\mathbf{v} \\right\\}$ $\\left\\{ \\mathbf{m}_{l_2,h} : h \\in \\left\\{ 0, 1, \\cdots , H-1 \\right\\} \\right\\}$ along axis 0 to get\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{l_2} = \\begin{bmatrix} \\mathbf{m}_{l_2,0} \\\\ \\mathbf{m}_{l_2,1} \\\\ \\vdots \\\\ \\mathbf{m}_{l_2,H-1} \\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "**Do the following tasks (Reasoning is not required).**\n",
    "\n",
    "1. What is the shape of $\\mathbf{W}^{\\mathbf{M}}$ for $\\mathbf{M} \\in \\left\\{ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right\\}$?\n",
    "2. What is the shape of $\\mathbf{q}_{l_1}$?\n",
    "3. What is the relationship between $\\mathbf{q}_{l_1}$ and $\\mathbf{W}^{\\mathbf{Q}}$?\n",
    "4. For $\\mathbf{m} \\in \\left\\{ \\mathbf{k}, \\mathbf{v} \\right\\}$, what is the shape of $\\mathbf{m}_{l_2}$?\n",
    "5. What is the relationship between $\\mathbf{m}_{l_2}$ and $\\mathbf{W}^{\\mathbf{M}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd386714-6bf3-4d1b-88ee-227ea18f5b1e",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. The shape of $\\mathbf{W}^{\\mathbf{Q}}$ is $(H\\cdot D_{qk},D_1)$. \\\n",
    "   The shape of $\\mathbf{W}^{\\mathbf{K}}$ is $(H\\cdot D_{qk},D_2)$. \\\n",
    "   The shape of $\\mathbf{W}^{\\mathbf{V}}$ is $(H\\cdot D_{v},D_2)$.\n",
    "2. The shape of $\\mathbf{q}_{l_1}$ is $(H\\cdot D_{qk},)$.\n",
    "3. $\\mathbf{q}_{l_1} = \\mathbf{W}^{\\mathbf{Q}}\\mathbf{x}_{l_1}$.\n",
    "4. The shape of $\\mathbf{k}_{l_2}$ is $(H\\cdot D_{qk},)$. \\\n",
    "   The shape of $\\mathbf{v}_{l_2}$ is $(H\\cdot D_{v},)$.\n",
    "5. $\\mathbf{m}_{l_2} = \\mathbf{W}^{\\mathbf{M}}\\mathbf{y}_{l_2}$.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea18b3c-336e-412b-9188-7666738c2bfe",
   "metadata": {},
   "source": [
    "## Part 3 (10 points, non-coding task)\n",
    "\n",
    "Define function $\\text{Softmax}: \\Bbb R^d \\rightarrow \\Bbb R^d$, with the $i$th output value as\n",
    "\n",
    "$$\n",
    "\\text{Softmax}_i \\left( \\mathbf{z} \\right) = \\frac{\\exp \\left( z_i \\right)}{\\sum_{j=0}^{d-1} \\exp \\left( z_j \\right)} .\n",
    "$$\n",
    "\n",
    "At position $l_1$ in the attending sequence, its attention score to position $l_2$ in the being attended sequence for head $h$ is denoted as $\\alpha_{h, l_1 l_2}$.\n",
    "\n",
    "We can write $\\alpha_{h, l_1 l_2}$ in the following form:\n",
    "\n",
    "$$\n",
    "\\alpha_{h, l_1 l_2} = \\text{Softmax}_{l_2} \\left( {\\color{red} \\boxed{???}} \\right) ,\n",
    "$$\n",
    "\n",
    "**What is the formula in the above red box (reasoning is not required)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d360a2c-d2c4-4db1-9f1a-5044b348cb3d",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "\\mathbf{K}_h = \\begin{bmatrix} \\mathbf{k}_{0,h}^\\top \\\\ \\mathbf{k}_{1,h}^\\top \\\\ \\vdots \\\\ \\mathbf{k}_{L_2-1,h}^\\top\\end{bmatrix} ,\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\alpha_{h, l_1 l_2} = \\text{Softmax}_{l_2} \\left( \\frac{\\mathbf{K}_h \\mathbf{q}_{l_1,h}}{\\sqrt{D_{qk}}} \\right) .\n",
    "$$\n",
    "\n",
    "> Here, we divide by $\\sqrt{D_{qk}}$ for numerical stability.\n",
    ">\n",
    "> Let us consider a pair of query/key vectors, $\\mathbf{a}, \\mathbf{b} \\in \\mathbb R^{D_{qk}}$, where each component is drawn independently from a standard normal distribution,\n",
    "> $$a_i, b_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal N(0,1) .$$\n",
    ">\n",
    "> Their dot product is\n",
    "> $$\\mathbf{a}\\cdot \\mathbf{b} = \\sum_{i=0}^{D_{qk}} a_ib_i .$$\n",
    ">\n",
    "> Since $a_ib_i$ has $\\text{E}[a_ib_i]=0$ and $\\text{Var}[a_ib_i]=1$, it follows that\n",
    "> $$\\text{E}[\\mathbf{a}\\cdot \\mathbf{b}]=0, \\quad \\text{Var}[\\mathbf{a}\\cdot \\mathbf{b}] = \\sum_{i=0}^{D_{qk}} \\text{Var}[a_ib_i]=D_{qk} .$$\n",
    ">\n",
    "> Thus, the standard deviation of the dot product grows with $\\sqrt{D_{qk}}$. Here comes the factor in the denominator.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c5567-e987-4200-b9c2-e56890259bb6",
   "metadata": {},
   "source": [
    "## Part 4 (5 points, non-coding task)\n",
    "\n",
    "At position $l_1$ in an attending sequence, for head $h$, the information extracted from attending to a being attended sequence is given by \n",
    "\n",
    "$$\n",
    "\\mathbf{o}_{h,l_1} = \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{v}_{l_2,h} .\n",
    "$$\n",
    "\n",
    "We hereafter call $\\mathbf{o}_{h,l_1}$ a **pre-out-projection output vector**.\n",
    "\n",
    "**Do the following tasks.**\n",
    "\n",
    "1. What is the shape of vector $\\mathbf{o}_{h,l_1}$?\n",
    "2. We concatenate $\\left\\{\\mathbf{o}_{h,l_1} : h \\in \\left\\{ 0, 1 , \\cdots , H-1 \\right\\} \\right\\}$ along axis 0: $$\\mathbf{o}_{l_1} = \\begin{bmatrix} \\mathbf{o}_{0,l_1} \\\\ \\mathbf{o}_{1,l_1} \\\\ \\vdots \\\\ \\mathbf{o}_{H-1,l_1} \\end{bmatrix}$$ \\\n",
    "   What is the shape of $\\mathbf{o}_{l_1}$?\n",
    "3. We project $\\mathbf{o}_{l_1}$ to a **post-out-projection output vector** via an out-projection matrix: $$\\mathbf{x}_{l_1}^{out} = \\mathbf{W}^O \\mathbf{o}_{l_1} \\in \\Bbb R^{D_1} ,$$ where $$\\mathbf{W}^O = \\begin{bmatrix} \\mathbf{W}^O_0 & \\mathbf{W}^O_1 & \\cdots & \\mathbf{W}^O_{H-1} \\end{bmatrix}.$$ \\\n",
    "   What is the shape of $\\mathbf{W}^O_h$ for each $h \\in \\left\\{ 0 , 1 , \\cdots , H-1 \\right\\}$ and $\\mathbf{W}^O$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842add8-5cc3-4dc2-ac47-0f0b277398a9",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. $\\alpha_{h, l_1 l_2}$ is a scalar; the shape follows $\\mathbf{v}_{l_2,h}$, which is $(D_v,)$.\n",
    "2. The shape is $(H\\cdot D_v,)$.\n",
    "3. $\\mathbf{W}^O$ projects from $\\Bbb R^{H\\cdot D_v}$ to $\\Bbb R^{D_1}$. Therefore, it has shape $(D_1, H\\cdot D_v)$. \\\n",
    "   The shape of $\\mathbf{W}^O_h$ is $(D_1,D_v)$.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d79d3-2e7d-4683-be40-d251aee8512a",
   "metadata": {},
   "source": [
    "## Part 5 (10 points, coding task)\n",
    "\n",
    "**In this part, you are asked to build your own multi-head attention module that subclasses `nn.Module`.**\n",
    "\n",
    "- For simplicity, we ignore any masking. That is, each position in an attending sequence attends to all positions in a being attended sequence.\n",
    "- In your code, you do not need to worry about whether your code is efficient in an autoprogressive token generation process when your module is used in inference in a GPT-like task. \\\n",
    "  That is, if we use your code in a GPT-like task to autoprogressively generate tokens, it is totally fine if you repeatly generate the same key and value at a given position rather than more efficiently storing their values in cache.\n",
    "- The class name is `MyMHA`.\n",
    "- Attributes:\n",
    "  - `D_1`: Dimension of a hidden state/token in an attending sequence.\n",
    "  - `D_2`: Dimension of a hidden state/token in a being attended sequence.\n",
    "  - `D_v`: Dimension of a value vector.\n",
    "  - `D_qk`: Dimension of a query/key vector.\n",
    "  - `H`: Number of heads.\n",
    "  - `W_Q`: A linear module whose weight is a query-projection matrix. The shape should be consistent with your answer in Part 2. No bias.\n",
    "  - `W_K`: A linear module whose weight is a key-projection matrix. The shape should be consistent with your answer in Part 2. No bias.\n",
    "  - `W_V`: A linear module whose weight is a value-projection matrix. The shape should be consistent with your answer in Part 2. No bias.\n",
    "  - `W_O`: A linear module whose weight is an out-projection matrix. The shape should be consistent with your answer in Part 4. No bias.\n",
    "- Method `__init__`:\n",
    "  - Inputs:\n",
    "    - `D_1`\n",
    "    - `D_2`\n",
    "    - `D_qk`\n",
    "    - `D_v`\n",
    "    - `H`\n",
    "  - Outputs:\n",
    "    - None.\n",
    "  - What to do inside this method:\n",
    "    - Initialize attribute values.\n",
    "- Method `forward`:\n",
    "  - Inputs:\n",
    "    - An attending sequence (tensor) with shape `(B,L_1,D_1)`.\n",
    "    - A being attended sequence (tensor) with shape `(B,L_2,D_2)`.\n",
    "  - Outputs:\n",
    "    - Post-out-projection outputs with shape `(B,L_1,D_1)`.\n",
    "  - What to do inside this method:\n",
    "    - Compute the outputs.\n",
    "    - After each operation, add a comment on the tensor shape.\n",
    "    - **Do not use any loop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23040618-87c2-475f-80ce-2d1b02243fac",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77295ea7-767b-40c7-bf05-6a83e9dd78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMHA(nn.Module):\n",
    "    def __init__(self, D_1, D_2, D_qk, D_v, H):\n",
    "        super().__init__()\n",
    "        self.D_1 = D_1\n",
    "        self.D_2 = D_2\n",
    "        self.D_qk = D_qk\n",
    "        self.D_v = D_v\n",
    "        self.H = H\n",
    "        self.W_Q = nn.Linear(D_1, H * D_qk, bias=False)\n",
    "        self.W_K = nn.Linear(D_2, H * D_qk, bias=False)\n",
    "        self.W_V = nn.Linear(D_2, H * D_v, bias=False)\n",
    "        self.W_O = nn.Linear(H * D_v, D_1, bias=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        L_1, L_2 = x.shape[1], y.shape[1]\n",
    "        \n",
    "        Q = self.W_Q(x)  # (B, L_1, H*D_qk)\n",
    "        K = self.W_K(y)  # (B, L_2, H*D_qk)\n",
    "        V = self.W_V(y)  # (B, L_2, H*D_v)\n",
    "        Q = Q.reshape(-1, L_1, self.H, self.D_qk).permute(0, 2, 1, 3)  # (B, H, L_1, D_qk)\n",
    "        K = K.reshape(-1, L_2, self.H, self.D_qk).permute(0, 2, 1, 3)  # (B, H, L_2, D_qk)\n",
    "        V = V.reshape(-1, L_2, self.H, self.D_v).permute(0, 2, 1, 3)  # (B, H, L_2, D_v)\n",
    "\n",
    "        logits = Q @ K.mT / self.D_qk ** 0.5  # (B, H, L_1, L_2)\n",
    "        Alpha = nn.functional.softmax(logits, dim=-1)  # (B, H, L_1, L_2)\n",
    "\n",
    "        O = Alpha @ V  # (B, H, L_1, D_v)\n",
    "        O = O.permute(0, 2, 1, 3).reshape(-1, L_1, self.H * self.D_v)  # (B, L_1, H*D_v)\n",
    "        \n",
    "        x_out = self.W_O(O)  # (B, L_1, D_1)\n",
    "        assert x_out.shape == x.shape\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa260467-8a20-4f05-ae47-2ce80bd4a19f",
   "metadata": {},
   "source": [
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c83f24-29e1-4997-af87-b903b679f529",
   "metadata": {},
   "source": [
    "Next, let us study a variant of MHA: **Group Query Attention (GQA)**.\n",
    "\n",
    "Recall that in MHA, the number of heads in queries, keys and values are the same, $H$. Thus, query $\\mathbf{q}_{l_1, h}$ attends to key $\\mathbf{k}_{l_2, h}$ with the same head index $h$.\n",
    "\n",
    "In GQA, we relax this constraint by allowing keys and values to have $G$ heads ($G\\le H$), where $G$ is a factor of $H$. For instance, if $H=12$, then $G \\in \\left\\{ 1, 2, 3, 4, 6, 12 \\right\\}$.\n",
    "\n",
    "In GQA, a query $\\mathbf{q}_{l_1, \\color{red}{h}}$ with head $\\color{red}{h}$ is permitted to attend to a key $\\mathbf{k}_{l_2, \\color{blue}{g}}$ and use value $\\mathbf{v}_{l_2, \\color{blue}{g}}$ in computing its output with head $\\color{blue}{g}$ if\n",
    "\n",
    "$$\n",
    "{\\color{red}{h}} \\equiv {\\color{blue}{g}} \\pmod{G} .\n",
    "$$\n",
    "\n",
    "Thus, each head in keys and values is mapped to $\\frac{H}{G} \\geq 1$ heads in queries.\n",
    "\n",
    "As an example, suppose $H=12$ and $G=3$. Then\n",
    "\n",
    "- Head ${\\color{blue}{g}} = 0$ in keys and values is associated with heads ${\\color{red}{h}} = 0, 3, 6, 9$ in queries.\n",
    "- Head ${\\color{blue}{g}} = 1$ in keys and values is associated with heads ${\\color{red}{h}} = 1, 4, 7, 10$ in queries.\n",
    "- Head ${\\color{blue}{g}} = 2$ in keys and values is associated with heads ${\\color{red}{h}} = 2, 5, 8, 11$ in queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fe468-1ad7-40d2-86d8-bb1baf020ebe",
   "metadata": {},
   "source": [
    "## Part 6 (5 points, non-coding task)\n",
    "\n",
    "For $\\mathbf{M} \\in \\left\\{ \\mathbf{K}, \\mathbf{V} \\right\\}$, denote the $\\mathbf{M}$-projection matrix as \n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{\\mathbf{M}, GQA} = \\begin{bmatrix} \\mathbf{W}^{\\mathbf{M}, GQA}_0 \\\\ \\vdots \\\\ \\mathbf{W}^{\\mathbf{M}, GQA}_{G-1} \\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "Now, we concatenate $\\frac{H}{G}$ copies of the above matrix along axis 0:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde W}^{\\mathbf{M}, GQA} = \\begin{bmatrix} \\mathbf{W}^{\\mathbf{M}, GQA} \\\\ \\mathbf{W}^{\\mathbf{M}, GQA} \\\\ \\vdots \\\\ \\mathbf{W}^{\\mathbf{M}, GQA} \\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "**What is the relationship between $\\text{rank} \\left( \\mathbf{\\tilde W}^{\\mathbf{M}, GQA} \\right)$ and $\\text{rank} \\left( \\mathbf{W}^{\\mathbf{M}, GQA} \\right)$?**\n",
    "\n",
    "- Reasoning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff70774-0175-4791-a701-014161bf47e2",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "When we vertically stack identical copies of a matrix, we are repeating the same row space multiple times. The set of linearly independent rows does not increase. Thus,\n",
    "\n",
    "$$\n",
    "\\text{rank} \\left( \\mathbf{\\tilde W}^{\\mathbf{M}, GQA} \\right) = \\text{rank} \\left( \\mathbf{W}^{\\mathbf{M}, GQA} \\right) .\n",
    "$$\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5854f28-41df-4f77-9f13-ed450d56b774",
   "metadata": {},
   "source": [
    "## Part 7 (10 points, coding task)\n",
    "\n",
    "**In this part, please build your own GQA module called `MyGQA`.**\n",
    "\n",
    "- The requirement is pretty much the same as Part 5.\n",
    "- Do NOT create $H/G$ copies of key-projection and value-projection matrices. Otherwise, you will use too much unnecessary memory.\n",
    "- No loop is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df184a-27c0-4d2c-9308-423274fc7148",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b2e34c-3f5c-4bb5-8303-12f3081f8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGQA(nn.Module):\n",
    "    def __init__(self, D_1, D_2, D_qk, D_v, H, G):\n",
    "        super().__init__()\n",
    "        self.D_1 = D_1\n",
    "        self.D_2 = D_2\n",
    "        self.D_qk = D_qk\n",
    "        self.D_v = D_v\n",
    "        self.H = H\n",
    "        self.G = G\n",
    "        self.W_Q = nn.Linear(D_1, H * D_qk, bias=False)\n",
    "        self.W_K = nn.Linear(D_2, G * D_qk, bias=False)\n",
    "        self.W_V = nn.Linear(D_2, G * D_v, bias=False)\n",
    "        self.W_O = nn.Linear(H * D_v, D_1, bias=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        L_1, L_2 = x.shape[1], y.shape[1]\n",
    "        num_copies = self.H // self.G\n",
    "        \n",
    "        Q = self.W_Q(x)  # (B, L_1, H*D_qk)\n",
    "        K = self.W_K(y)  # (B, L_2, G*D_qk)\n",
    "        V = self.W_V(y)  # (B, L_2, G*D_v)\n",
    "        Q = Q.reshape(-1, L_1, num_copies, self.G, self.D_qk).permute(0, 2, 3, 1, 4)  # (B, num_copies, G, L_1, D_qk)\n",
    "        K = K.reshape(-1, L_2, 1, self.G, self.D_qk).permute(0, 2, 3, 1, 4)  # (B, 1, G, L_2, D_qk)\n",
    "        V = V.reshape(-1, L_2, 1, self.G, self.D_v).permute(0, 2, 3, 1, 4)  # (B, 1, G, L_2, D_v)\n",
    "\n",
    "        logits = Q @ K.mT / self.D_qk ** 0.5  # (B, num_copies, G, L_1, L_2)\n",
    "        Alpha = nn.functional.softmax(logits, dim=-1)  # (B, num_copies, G, L_1, L_2)\n",
    "\n",
    "        O = Alpha @ V  # (B, num_copies, G, L_1, D_v)\n",
    "        O = O.permute(0, 3, 1, 2, 4).reshape(-1, L_1, self.H * self.D_v)  # (B, L_1, H*D_v)\n",
    "        \n",
    "        x_out = self.W_O(O)  # (B, L_1, D_1)\n",
    "        assert x_out.shape == x.shape\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837a218-af6f-4922-98a2-9be67b9f33c1",
   "metadata": {},
   "source": [
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca69ce-d24a-47c5-8a38-7b707c4dca35",
   "metadata": {},
   "source": [
    "## Part 8 (5 points, non-coding task)\n",
    "\n",
    "**MHA is a special case of GQA. Explain why.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b84c0-1b9c-4d28-8f3e-67d8ebf35bb8",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "When we let $G=H$, GQA is exactly MHA.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2672563-7085-4f1f-9b75-4a45a518a748",
   "metadata": {},
   "source": [
    "Now, let us study another variant of MHA: **Multi-head Latent Attention (MLA)**. MLA was introduced by **DeepSeek**. It is a core component of DeepSeek’s large language model (LLM).\n",
    "\n",
    "The key intuition of MLA is as follows. In MHA, the key and value projection matrices\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{\\mathbf{K}, MHA} \\in \\Bbb R^{H \\cdot D_{qk} \\times D_2} , \\quad \\mathbf{W}^{\\mathbf{V}, MHA} \\in \\Bbb R^{H \\cdot D_v \\times D_2}\n",
    "$$\n",
    "\n",
    "may be high dimensional.\n",
    "\n",
    "For instance, suppose $H \\cdot D_{qk} = H \\cdot D_v = D_2 = 4096$.\n",
    "\n",
    "However, it is not necessarily the case that these matrices are of high ranks (such as 4096). Their actual ranks (or top few ranks that make their truncated singular value decomposition (SVD) to be close to the actual matrices) may be much lower than that.\n",
    "\n",
    "To capture the low-rank feature, MLA proposed the following model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}^{\\mathbf{K}, MHA} & = {\\color{blue}{\\mathbf{W}^{\\mathbf{UK}, MLA}}} {\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} \\\\ {\\mathbf{W}^{\\mathbf{V}, MHA}} & = {\\color{green}{\\mathbf{W}^{\\mathbf{UV}, MLA}}} {\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "\n",
    "- ${\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} \\in \\Bbb R^{r \\times D_2}$: down-projection matrix for computing keys and values.\n",
    "- ${\\color{blue}{\\mathbf{W}^{\\mathbf{UK}, MLA}}} \\in \\Bbb R^{H \\cdot D_{qk} \\times r}$: up-projection matrix for computing keys.\n",
    "- ${\\color{green}{\\mathbf{W}^{\\mathbf{UV}, MLA}}} \\in \\Bbb R^{H \\cdot D_v \\times r}$: up-projection matrix for computing values.\n",
    "\n",
    "In practice, rank $r$ is typically much smaller than $\\min \\left\\{ H \\cdot D_{qk} , H \\cdot D_v , D_2 \\right\\}$.\n",
    "\n",
    "**In all remaining parts of this problem, to simplify your analysis and highlight the relationships of MHA, GQA and MLA, we make the following assumptions:**\n",
    "\n",
    "- $D_1 = D_2 = D$.\n",
    "- $D_{qk} = D_v = d$.\n",
    "- $d$ is a factor of $D$.\n",
    "\n",
    "Under these assumptions, the number heads $H$ satisfies\n",
    "\n",
    "$$\n",
    "H = \\frac{D}{d}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265adf7-e95a-410b-84d8-9ae1c26442cd",
   "metadata": {},
   "source": [
    "## Part 9 (10 points, non-coding task)\n",
    "\n",
    "**In this part, you are asked to prove that GQA can be equivalently represented by MLA.**\n",
    "\n",
    "In your solution, it is sufficient for you to prove that for $\\mathbf{M} \\in \\left\\{ \\mathbf{K}, \\mathbf{V} \\right\\}$, for matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde W}^{\\mathbf{M}, GQA} = \\begin{bmatrix} \\color{orange}{\\mathbf{W}^{\\mathbf{M}, GQA}} \\\\ \\color{orange}{\\mathbf{W}^{\\mathbf{M}, GQA}} \\\\ \\vdots \\\\ \\color{orange}{\\mathbf{W}^{\\mathbf{M}, GQA}} \\end{bmatrix} \\in \\Bbb R^{D \\times D}\n",
    "$$\n",
    "\n",
    "(defined in Part 6) who is the concatenation of $\\frac{H}{G}$ copies of\n",
    "\n",
    "$$\n",
    "{\\color{orange}{\\mathbf{W}^{\\mathbf{M}, GQA}}} = \\begin{bmatrix} \\mathbf{W}^{\\mathbf{M}, GQA}_0 \\\\ \\vdots \\\\ \\mathbf{W}^{\\mathbf{M}, GQA}_{G-1} \\end{bmatrix} \\in \\Bbb R^{G \\cdot d \\times D}\n",
    "$$\n",
    "\n",
    "matrix $\\mathbf{\\tilde W}^{\\mathbf{M}, GQA}$ can be decomposed as\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde W}^{\\mathbf{M}, GQA} = \\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}} \\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- ${\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} \\in \\Bbb R^{r \\times D}$: down-projection matrix for computing keys and values.\n",
    "- ${\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}}} \\in \\Bbb R^{D \\times r}$: up-projection matrix for computing $\\mathbf{M}$ (keys or values).\n",
    "- $r = G \\cdot d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947a722-3544-4c17-9641-d52bebd8e37b",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "Follow Part 6, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{rank} \\left( \\mathbf{\\tilde W}^{\\mathbf{M}, GQA} \\right) &= \\text{rank} \\left( \\mathbf{W}^{\\mathbf{M}, GQA} \\right) \\\\ \n",
    "                                                               &\\leq \\min \\left\\{ G \\cdot d, D \\right\\} \\\\ \n",
    "                                                               &= \\min \\left\\{ r, D \\right\\} \\\\\n",
    "                                                               &= r.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "That means it is sufficient to keep the first $r$ singular vectors/values after SVD. Thus, $\\mathbf{\\tilde W}^{\\mathbf{M}, GQA}$ can be decomposed into\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{\\tilde W}^{\\mathbf{M}, GQA} &= \\sum_{i=0}^{r-1} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top \\\\\n",
    "                                    &= \\underbrace{\\begin{bmatrix} \\mathbf{u}_0 & \\mathbf{u}_1 & \\cdots & \\mathbf{u}_{r-1} \\end{bmatrix}}_{\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}}} \\underbrace{\\begin{bmatrix} \\sigma_0 \\mathbf{v}_0^\\top \\\\ \\sigma_1 \\mathbf{v}_1^\\top \\\\ \\vdots \\\\ \\sigma_{r-1} \\mathbf{v}_{r-1}^\\top \\end{bmatrix}}_{\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be8a2f-81f4-43d5-b2bc-4ca41614653c",
   "metadata": {},
   "source": [
    "## Part 10 (5 points, coding task)\n",
    "\n",
    "This question follows Part 9.\n",
    "\n",
    "**You are asked to define a function called `GQA_2_MLA` that performs the following tasks:**\n",
    "\n",
    "- Input:\n",
    "  - `W_M_GQA`: A numpy array with shape `(r,D)`, where `r` is guaranteed to be a factor of `D` (not something you need to worry about).\n",
    "- Outputs:\n",
    "  - `W_DKV_MLA`: A numpy array with shape `(r,D)`.\n",
    "  - `W_UM_MLA`: A numpy array with shape `(D,r)`.\n",
    "- Things to do inside this function:\n",
    "  - Compute `W_M_GQA_tilde` that concatenates `D/r` copies of `W_M_GQA` along axis 0.\n",
    "  - Print the shapes of `W_UM_MLA` and `W_DKV_MLA`.\n",
    "  - Print the mean squared error between `W_M_GQA_tilde` and `W_UM_MLA @ W_DKV_MLA`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You may use `np.linalg`.\n",
    "- PyTorch is not allowed.\n",
    "- No loop in your code.\n",
    "\n",
    "**After defining this function, test it with the input `np.random.randn(4,24)`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f56789-f4c9-4021-b153-8be8d9b55eff",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de44913-a3a9-47ab-8abe-7f6e2a2f4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GQA_2_MLA(W_M_GQA):\n",
    "    r, D = W_M_GQA.shape\n",
    "    \n",
    "    W_M_GQA_tilde = np.repeat(W_M_GQA, D/r, axis=0)\n",
    "\n",
    "    svd_result = np.linalg.svd(W_M_GQA_tilde)\n",
    "    W_UM_MLA = svd_result.U[:, :r]\n",
    "    W_DKV_MLA = svd_result.S.reshape(-1, 1)[:r, :] * svd_result.Vh[:r, :]\n",
    "    print(W_UM_MLA.shape)\n",
    "    print(W_DKV_MLA.shape)\n",
    "    \n",
    "    diff = W_M_GQA_tilde - W_UM_MLA @ W_DKV_MLA\n",
    "    mse = (diff ** 2).mean().item()\n",
    "    print(mse)\n",
    "    \n",
    "    return W_DKV_MLA, W_UM_MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46697fa-c809-441b-a1f3-59c101925386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4)\n",
      "(4, 24)\n",
      "9.395979459474246e-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-4.25411178e+00, -2.41889436e+00,  3.03645615e+00,\n",
       "          3.82686855e+00,  5.66765433e-01,  2.07039774e+00,\n",
       "         -5.04391489e+00, -3.19270099e+00, -2.97260926e+00,\n",
       "         -3.59589403e+00,  6.01044194e+00, -3.63756465e+00,\n",
       "          3.48198567e-01,  7.98484009e-01,  5.19685066e+00,\n",
       "          4.00259170e+00, -7.07379559e+00,  3.57480729e+00,\n",
       "          6.66131817e-02,  3.33952287e-01, -4.59786885e+00,\n",
       "         -1.69744675e+00, -2.52661298e+00,  1.64497599e+00],\n",
       "        [-3.67740270e+00, -6.27618829e+00, -1.07549493e+00,\n",
       "          3.30033249e+00, -1.51136545e+00, -4.17523424e+00,\n",
       "          2.39789793e-01,  2.08411957e-01, -1.41500075e+00,\n",
       "          2.73544092e+00, -2.50170177e+00, -2.55102427e+00,\n",
       "          1.15177154e-01, -1.60434363e+00,  2.10904595e+00,\n",
       "         -1.86395880e+00,  1.17979159e+00, -4.00804472e+00,\n",
       "          1.09562465e+00, -1.73634402e+00, -3.70922923e-01,\n",
       "         -4.92446517e+00,  2.82167645e+00,  3.40133175e-01],\n",
       "        [ 4.08518986e-01, -4.37337348e+00, -1.00687922e+00,\n",
       "         -6.73821613e-01, -3.14367802e+00, -1.32461371e+00,\n",
       "         -4.91240342e+00, -1.19290464e+00,  2.25398974e+00,\n",
       "         -3.04706861e+00, -2.25029578e-01,  8.24240917e-01,\n",
       "          2.76166476e+00,  2.88034896e+00, -2.16543300e-01,\n",
       "          4.02950465e+00,  2.57239067e+00,  3.89061497e-01,\n",
       "         -2.50700030e+00,  1.27170727e-01,  4.57959103e+00,\n",
       "          1.86692145e+00,  1.47849144e+00, -3.32148066e-01],\n",
       "        [-3.46774141e+00, -1.26759710e-01, -1.60039983e-01,\n",
       "         -2.36543240e-01,  3.22029474e+00,  1.52886888e+00,\n",
       "          6.74252125e-01, -1.54068058e+00,  4.45927396e-01,\n",
       "          1.13285041e+00, -1.94438798e+00, -2.03906869e-01,\n",
       "         -1.47269670e+00, -1.29033631e+00, -1.31463012e+00,\n",
       "          2.48429408e+00,  2.28713697e+00,  5.28660866e+00,\n",
       "         -1.26493467e+00,  1.59413067e-03,  5.20799396e-01,\n",
       "         -2.04744418e+00,  2.39110548e+00, -1.87010332e+00]]),\n",
       " array([[ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [ 0.07705649, -0.11287244,  0.27903067, -0.26482195],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [-0.31647197,  0.16215031, -0.03450809, -0.19755664],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [ 0.24460943,  0.27121718, -0.10161363, -0.15148864],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935],\n",
       "        [-0.02721548,  0.23254032,  0.27801514,  0.18589935]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GQA_2_MLA(np.random.randn(4, 24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc1916-8ee9-42f1-a9b4-60e9393dd27c",
   "metadata": {},
   "source": [
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8467f7b-f368-4522-be32-d78c5e04d533",
   "metadata": {},
   "source": [
    "## Part 11 (10 points, non-coding task)\n",
    "\n",
    "So far, we have proved that GQA can always be represented by MLA.\n",
    "\n",
    "**In this part, you are asked to prove that GQA is not equivalent to MLA. What you need to do is to find one example that MLA cannot be represented as GQA.**\n",
    "\n",
    "To be specific, please do the following things:\n",
    "\n",
    "1. Construct ${\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} \\in \\Bbb R^{1 \\times 2}$.\n",
    "2. Construct ${\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}}} \\in \\Bbb R^{2 \\times 1}$.\n",
    "3. Do matrix multiplication $\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}} \\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}$.\n",
    "4. Show that this product matrix is not the concatenation of two copies of 1-by-2 matrices along axis 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770ac0c-ae9d-41b1-917e-928b9deb5bee",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. Choose ${\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} = \\begin{bmatrix} 1 & 2 \\end{bmatrix}$.\n",
    "2. Choose ${\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}}} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n",
    "3. ${\\color{blue}{\\mathbf{W}^{\\mathbf{UM}, MLA}}} {\\color{red}{\\mathbf{W}^{\\mathbf{DKV}, MLA}}} = \\begin{bmatrix} 2 & 1 \\\\ 4 & 2 \\end{bmatrix}$.\n",
    "4. The product is not in the form of $\\begin{bmatrix} a & b \\\\ a & b \\end{bmatrix}$. Shown.\n",
    "\n",
    "\n",
    "In other words, **MLA is strictly more expressive than GQA**.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835ffe0-978f-4b82-8481-6ccc9cfd7bba",
   "metadata": {},
   "source": [
    "MLA does not only enjoy its advantage of being more general than MHA and GQA, it is also computationally more efficient.\n",
    "\n",
    "An intuitive approach of computing MLA.\n",
    "\n",
    "1. Compute the key-projection matrix $\\mathbf{W}^{\\mathbf{UK}, MLA} \\mathbf{W}^{\\mathbf{DKV}, MLA} \\in \\Bbb R^{D \\times D}$ and the value-projection matrix $\\mathbf{W}^{\\mathbf{UV}, MLA} \\mathbf{W}^{\\mathbf{DKV}, MLA} \\in \\Bbb R^{D \\times D}$.\n",
    "2. Follow the standard steps in MHA.\n",
    "\n",
    "This approach is hereafter called a **vanilla approach**. This approach fails to enjoy the low-rank feature of $\\mathbf{W}^{\\mathbf{DKV}, MLA}$, $\\mathbf{W}^{\\mathbf{UK}, MLA}$, and $\\mathbf{W}^{\\mathbf{UV}, MLA}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975de6db-1815-46a4-b41b-580c7758cb1b",
   "metadata": {},
   "source": [
    "## Part 12 (10 points, non-coding task)\n",
    "\n",
    "**In this part, you are asked to study an alternative approach to compute MLA.**\n",
    "\n",
    "1. Find a **head-independent reduced key-projection matrix** ${\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}} \\in \\Bbb R^{r \\times D}$ and a **reduced query-projection matrix** ${\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}}} \\in \\Bbb R^{H \\cdot r \\times D}$, such that\n",
    "   - The **reduced key** at position $l_2$ for head $h$ in a being attended sequence is head-independent and is given by:\n",
    "     $$\\mathbf{\\hat k}_{l_2} = {\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}} \\mathbf{y}_{l_2} \\in \\Bbb R^r .$$\n",
    "   - The **reduced query** at position $l_1$ for head $h$ in an attending sequence is given by:\n",
    "     $$\\mathbf{\\hat q}_{l_1, h} = {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}_h}} \\mathbf{x}_{l_1} \\in \\Bbb R^r ,$$\n",
    "     where\n",
    "     $${\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}}} = \\begin{bmatrix} {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}_0}} \\\\ {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}_1 }} \\\\ \\vdots \\\\ {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}_{H-1}}} \\end{bmatrix} .$$\n",
    "   - The attention score (query-key similarity) is invariant in both the original and the reduced forms. That is\n",
    "     $$\\frac{\\mathbf{q}_{l_1,h}^\\top \\mathbf{k}_{l_2,h}}{\\sqrt{D/H}} = \\frac{\\mathbf{\\hat q}_{l_1,h}^\\top \\mathbf{\\hat k}_{l_2}}{\\sqrt{r}} . \\quad (1)$$\n",
    "2. Find a **head-independent reduced value-projection matrix** ${\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}} \\in \\Bbb R^{r \\times D}$ and a **reduced out-projection matrix** ${\\color{orange}{\\hat {\\mathbf W}^{O, MLA}}} \\in \\Bbb R^{D \\times H \\cdot r}$, such that\n",
    "   - The **reduced value** with head $h$ on position $l_2$ in a being attended sequence is head-independent and is given by:\n",
    "     $$\\mathbf{\\hat v}_{l_2} = {\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}} \\mathbf{y}_{l_2} \\in \\Bbb R^r .$$\n",
    "   - Post-out-projection is invariant in both the original and the reduced forms. \\\n",
    "     Let\n",
    "     $${\\color{orange}{\\hat {\\mathbf W}^{O, MLA}}} = \\begin{bmatrix} {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_0}} & {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_1}} & \\cdots & {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_{H-1}}} \\end{bmatrix} ,$$\n",
    "     then we must have\n",
    "     $$\\sum_{h=0}^{H-1} \\mathbf W^O_h \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{v}_{l_2,h} = \\sum_{h=0}^{H-1} {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_h}} \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{\\hat v}_{l_2} . \\quad (2)$$\n",
    "\n",
    "Your answer of $\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}$, $\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}$, $\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}}$, and $\\color{orange}{\\hat {\\mathbf W}^{O, MLA}}$ should be written in terms of $\\mathbf{W}^{\\mathbf{DKV}}$, $\\mathbf{W}^{\\mathbf{UK}}$, $\\mathbf{W}^{\\mathbf{UV}}$, $\\mathbf{W}^{\\mathbf{Q}}$, and $\\mathbf{W}^O$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce921a-04eb-431b-8065-f9bbe36a200f",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. From (1),\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\frac{\\mathbf{q}_{l_1,h}^\\top \\mathbf{v}_{l_2,h}}{\\sqrt{D/H}} &= \\frac{\\mathbf{\\hat q}_{l_1,h}^\\top \\mathbf{\\hat v}_{l_2}}{\\sqrt{r}} \\\\\n",
    "   \\frac{\\left( \\mathbf{W}^{\\mathbf{Q}}_h \\mathbf{x}_{l_1} \\right)^\\top \\left( \\mathbf{W}^{\\mathbf{UK}}_h \\mathbf{W}^{\\mathbf{DKV}} \\mathbf{y}_{l_2} \\right)}{\\sqrt{D/(D/d)}} &= \\frac{\\left( {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}_h}} \\mathbf{x}_{l_1} \\right)^\\top \\left( {\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}} \\mathbf{y}_{l_2} \\right)}{\\sqrt{r}} \\\\\n",
    "   \\frac{\\mathbf{x}_{l_1}^\\top \\mathbf{W}^{\\mathbf{Q}, \\top}_h \\mathbf{W}^{\\mathbf{UK}}_h \\mathbf{W}^{\\mathbf{DKV}} \\mathbf{y}_{l_2}}{\\sqrt{d}} &= \\frac{\\mathbf{x}_{l_1}^\\top   {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA, \\top}_h}} {\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}} \\mathbf{y}_{l_2}}{\\sqrt{r}} .\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   We can set ${\\color{red}{\\hat {\\mathbf W}^{\\mathbf{K}, MLA}}} = \\mathbf{W}^{\\mathbf{DKV}}$ and ${\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA, \\top}_h}} = \\frac{1}{\\sqrt G} \\mathbf{W}^{\\mathbf{Q}, \\top}_h \\mathbf{W}^{\\mathbf{UK}}_h$. Thus,\n",
    "   $$\n",
    "   {\\color{blue}{\\hat {\\mathbf W}^{\\mathbf{Q}, MLA}}} = \\sqrt{\\frac{r}{d}} \\begin{bmatrix} \\mathbf{W}^{\\mathbf{UK}, \\top}_0 \\mathbf{W}^{\\mathbf{Q}}_0 \\\\ \\mathbf{W}^{\\mathbf{UK}, \\top}_1 \\mathbf{W}^{\\mathbf{Q}}_1 \\\\ \\vdots \\\\ \\mathbf{W}^{\\mathbf{UK}, \\top}_{H-1} \\mathbf{W}^{\\mathbf{Q}}_{H-1} \\end{bmatrix} .\n",
    "   $$\n",
    "2. From (2),\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\sum_{h=0}^{H-1} \\mathbf W^O_h \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{v}_{l_2,h} &= \\sum_{h=0}^{H-1} {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_h}} \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{\\hat v}_{l_2} \\\\\n",
    "   \\sum_{h=0}^{H-1} \\mathbf W^O_h \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf{W}^{\\mathbf{UV}}_h \\mathbf{W}^{\\mathbf{DKV}} \\mathbf{y}_{l_2} &= \\sum_{h=0}^{H-1} {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_h}} \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} {\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}} \\mathbf{y}_{l_2} \\\\\n",
    "   \\sum_{h=0}^{H-1} \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} \\mathbf W^O_h  \\mathbf{W}^{\\mathbf{UV}}_h \\mathbf{W}^{\\mathbf{DKV}} \\mathbf{y}_{l_2} &= \\sum_{h=0}^{H-1} \\sum_{l_2 = 0}^{L_2 - 1} \\alpha_{h, l_1 l_2} {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}_h}} {\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}} \\mathbf{y}_{l_2}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   We can set ${\\color{green}{\\hat {\\mathbf W}^{\\mathbf{V}, MLA}}} = \\mathbf{W}^{\\mathbf{DKV}}$ and ${\\color{orange}{\\hat {\\mathbf W}^{\\mathbf{O}, MLA}_h}} = \\mathbf{W}^{\\mathbf{O}}_h \\mathbf{W}^{\\mathbf{UV}}_h$. Thus,\n",
    "   $$\n",
    "   {\\color{orange}{\\hat {\\mathbf W}^{O, MLA}}} = \\begin{bmatrix} \\mathbf{W}^{\\mathbf{O}}_0 \\mathbf{W}^{\\mathbf{UV}}_0 & \\mathbf{W}^{\\mathbf{O}}_1 \\mathbf{W}^{\\mathbf{UV}}_1 & \\cdots & \\mathbf{W}^{\\mathbf{O}}_{H-1} \\mathbf{W}^{\\mathbf{UV}}_{H-1} \\end{bmatrix} .\n",
    "   $$\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c923f-7d3e-4bbf-be4a-e0ccb4185b33",
   "metadata": {},
   "source": [
    "## Part 13 (5 points, coding task)\n",
    "\n",
    "**Do the following tasks:**\n",
    "\n",
    "1. Define a function called `reduced_matrices`.\n",
    "   - Input arguments\n",
    "     - `W_DKV`, `W_UK`, `W_UV`, `W_Q`, `W_O`, `H`.\n",
    "   - Outputs\n",
    "     - `W_K_MLA_hat`, `W_V_MLA_hat`, `W_Q_MLA_hat`, `W_O_MLA_hat`.\n",
    "   - Requirement of your code:\n",
    "     - The code for computing each output must be in one line.\n",
    "     - Loop is not allowed.\n",
    "2. Set your device as `gpu`:\n",
    "   ```python\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   ```\n",
    "3. Construct the following synthetic data:\n",
    "   ```python\n",
    "   D = 1024\n",
    "   H = 32\n",
    "   D_qkv = D // H\n",
    "   r = 50\n",
    "    \n",
    "   W_DKV = torch.randn(r, D)\n",
    "   W_UK = torch.randn(D, r)\n",
    "   W_UV = torch.randn(D, r)\n",
    "   W_Q = torch.randn(D, D)\n",
    "   W_O = torch.randn(D, D)\n",
    "    \n",
    "   B = 32\n",
    "   L_1 = 100\n",
    "   L_2 = 300\n",
    "    \n",
    "   x = torch.randn(B, L_1, D).to(device)\n",
    "   y = torch.randn(B, L_2, D).to(device)\n",
    "   ```\n",
    "4. Study a vanilla attention model\n",
    "   - Initialize the model\n",
    "     ```python\n",
    "     model_MHA_vanilla = MyMHA(D, D, D_qkv, D_qkv, H)\n",
    "     ```\n",
    "   - Update model parameters\n",
    "     ```python\n",
    "     model_MHA_vanilla.W_K.weight, model_MHA_vanilla.W_V.weight, model_MHA_vanilla.W_Q.weight, model_MHA_vanilla.W_O.weight\n",
    "     ```\n",
    "   - Compute the output\n",
    "     ```python\n",
    "     output_vanilla = model_MHA_vanilla(x, y)\n",
    "     ```\n",
    "5. Study a reduced attention model\n",
    "   - Initialize the model\n",
    "     ```python\n",
    "     model_MHA_reduced = MyMHA(D, D, r, r, H)\n",
    "     ```\n",
    "   - Update model parameters\n",
    "     ```python\n",
    "     model_MHA_reduced.W_K.weight, model_MHA_reduced.W_V.weight, model_MHA_reduced.W_Q.weight, model_MHA_reduced.W_O.weight\n",
    "     ```\n",
    "   - Compute the output\n",
    "     ```python\n",
    "     output_reduced = model_MHA_reduced(x, y)\n",
    "     ```\n",
    "6. Check the correctness of the reduced model by computing and printing a relative error:\n",
    "   ```python\n",
    "   relative_error = mse_output**.5 / torch.mean(output_vanilla**2)**.5\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0262d-7cef-486f-a0ca-2ae5c8559598",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e003cbc1-8d32-4d66-83ab-5662036548cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_matrices(W_DKV, W_UK, W_UV, W_Q, W_O, H):\n",
    "    r, D = W_DKV.shape\n",
    "    d = D // H\n",
    "    \n",
    "    W_K_MLA_hat = W_DKV\n",
    "    W_V_MLA_hat = W_DKV\n",
    "    W_Q_MLA_hat = (W_UK.reshape(H, d, r).mT @ W_Q.reshape(H, d, D)).reshape(H * r, D) / (r / d) ** .5  # (H,r,d) @ (H,d,D) --> (H,r,D) --> (H*r,D)\n",
    "    W_O_MLA_hat = (W_O.reshape(D, H, d).permute(1, 0, 2) @ W_UV.reshape(H, d, r)).permute(1, 0, 2).reshape(D, H * r)  # (H,D,d) @ (H,d,r) --> (H,D,r) --> (D,H,r) --> (D,H*r)\n",
    "\n",
    "    return W_K_MLA_hat, W_V_MLA_hat, W_Q_MLA_hat, W_O_MLA_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f72a754-7394-46c5-a04d-584f09d712e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6af29e8-867b-48de-9fe5-04a94be4fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1024\n",
    "H = 32\n",
    "d = D // H\n",
    "r = 50\n",
    "\n",
    "W_DKV = torch.randn(r, D)\n",
    "W_UK = torch.randn(D, r)\n",
    "W_UV = torch.randn(D, r)\n",
    "W_Q = torch.randn(D, D)\n",
    "W_O = torch.randn(D, D)\n",
    " \n",
    "B = 32\n",
    "L_1 = 100\n",
    "L_2 = 300\n",
    " \n",
    "x = torch.randn(B, L_1, D).to(device)\n",
    "y = torch.randn(B, L_2, D).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3201a3-4fba-4001-9d63-e871732f9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MHA_vanilla = MyMHA(D, D, d, d, H)\n",
    "model_MHA_vanilla.W_K.weight = nn.Parameter(W_UK @ W_DKV)\n",
    "model_MHA_vanilla.W_V.weight = nn.Parameter(W_UV @ W_DKV)\n",
    "model_MHA_vanilla.W_Q.weight = nn.Parameter(W_Q)\n",
    "model_MHA_vanilla.W_O.weight = nn.Parameter(W_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae9552c-fa67-46b5-ab83-edc3193ae86e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_MHA_vanilla.to(device)\n",
    "output_vanilla = model_MHA_vanilla(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378a5d46-192e-4ec6-95f6-52c24138d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MHA_reduced = MyMHA(D, D, r, r, H)\n",
    "W_K_MLA_hat, W_V_MLA_hat, W_Q_MLA_hat, W_O_MLA_hat = reduced_matrices(W_DKV, W_UK, W_UV, W_Q, W_O, H)\n",
    "model_MHA_reduced.W_K.weight = nn.Parameter(W_K_MLA_hat)\n",
    "model_MHA_reduced.W_V.weight = nn.Parameter(W_V_MLA_hat)\n",
    "model_MHA_reduced.W_Q.weight = nn.Parameter(W_Q_MLA_hat)\n",
    "model_MHA_reduced.W_O.weight = nn.Parameter(W_O_MLA_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee34620-837e-4865-80b3-6affa488a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MHA_reduced.to(device)\n",
    "output_reduced = model_MHA_reduced(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5f3f71-df1d-4746-8467-ac153ff44745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2904, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_output = nn.functional.mse_loss(output_reduced, output_vanilla)\n",
    "relative_error = mse_output ** .5 / torch.mean(output_vanilla ** 2) ** .5  # Which is RMSE / RMS_vanilla\n",
    "relative_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd1b42-73ce-4914-aa38-7fb8928b268b",
   "metadata": {},
   "source": [
    "\"\"\" END OF THIS PART \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a667442-c564-44a9-9e38-4eb2241a4289",
   "metadata": {},
   "source": [
    "## Part 14 (5 points, non-coding task)\n",
    "\n",
    "In generative AI, such as GPT, we autoprogressively generate tokens. For a given position $l$, the keys and values on this position $\\mathbf{k}_l$ and $\\mathbf{v}_l$ are repeatedly used in generating tokens for positions $l' > l$.\n",
    "\n",
    "Therefore, the values of $\\mathbf{k}_l$ and $\\mathbf{v}_l$ are typically stored in cache (no need to revise your code in earlier parts if your code does not support this). We call such storage as $kv$-cache.\n",
    "\n",
    "**Do the following tasks to compute kv-cache in different models while doing autoregressive inference:** (reasoning is required)\n",
    "\n",
    "1. In MHA, the $kv$-cache at each position is $2D$. Explain why.\n",
    "2. In MLA, what is the $kv$-cache at each position?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267360d-956a-44bb-a55e-630b685a3862",
   "metadata": {},
   "source": [
    "\\#\\#\\# WRITE YOUR SOLUTION HERE ###\n",
    "\n",
    "1. $\\mathbf{k}_l,\\mathbf{v}_l \\in \\mathbb R^D$. Therefore, the $kv$-cache at each position is $2D$.\n",
    "2. $\\mathbf{\\hat k}_l,\\mathbf{\\hat v}_l \\in \\mathbb R^r$ and $\\mathbf{\\hat k}_l = \\mathbf{\\hat v}_l$ (remember we did `W_K_MLA_hat = W_DKV`, `W_V_MLA_hat = W_DKV` in the previous part). Therefore, the $kv$-cache at each position is only $r$.\n",
    "\n",
    "\"\"\" END OF THIS PART \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
