{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cffec3d-99fb-4239-838d-4a959b0cdfdf",
   "metadata": {},
   "source": [
    "# The Reality Filter\n",
    "\n",
    "> Without Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b01008-30b0-4134-b467-b6a9d7aa5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9368e364-9e25-4e70-a5eb-8c5c181b2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021e114-c943-4eb2-b24e-5e964b2b0d47",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d9804e-929f-4dad-b54a-0c12e903d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dir_, csv_filename=None, aggressive=False):\n",
    "        super().__init__()\n",
    "        if aggressive:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "        self.dir_ = Path(dir_)\n",
    "        assert self.dir_.exists() and self.dir_.is_dir(), \"image directory doesn't exist\"\n",
    "\n",
    "        self.images = sorted(self.dir_.glob(\"*.jpg\"))\n",
    "        self.labels = None\n",
    "        if csv_filename is not None:\n",
    "            self.labels = pd.read_csv(csv_filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.images[idx]\n",
    "        img = self.transform(Image.open(img_filename).convert(\"RGB\"))\n",
    "\n",
    "        label = torch.nan\n",
    "        if self.labels is not None:\n",
    "            label = self.labels.loc[self.labels[\"CodeID\"] == int(img_filename.stem), \"Label\"].values[0]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431fb37b-18ac-4824-bd92-f4fa10adecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ImageDataset(\"./train\", \"./train_data.csv\", aggressive=True)\n",
    "ds_test = ImageDataset(\"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a569a83c-6a39-4bbf-baf1-edc4167be26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=32)\n",
    "dl_test = DataLoader(ds_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcbb11-1b09-454d-804b-cfb32327da2f",
   "metadata": {},
   "source": [
    "## Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e073da8f-7bcb-4785-88f7-cdb08479a097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728, 486)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img129 = Image.open(\"./train/129.jpg\").convert(\"RGB\")\n",
    "width, height = img129.size\n",
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b0ad75-0508-42fb-bea9-92c0e4852993",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask1_rows = [(1, 0, width * height)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91203380-b7fb-41f5-9695-0e130489ab7e",
   "metadata": {},
   "source": [
    "## Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd018681-c3ed-426c-b7fb-f8964b082450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 93)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds_train.labels[\"Label\"]\n",
    "zeros, ones = labels[labels == 0].count().item(), labels[labels == 1].count().item()\n",
    "zeros, ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76aaad60-92c5-4985-b3d7-2fdbc581d062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d5f4fe-fdf5-453b-bfd6-e456c2f38806",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask2_rows = [(2, 0, min(zeros, ones) / max(zeros, ones))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171c7db-8f7e-4b12-9777-f75f18649a6c",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5c9db2-7cef-4a59-8c72-b9858e2f614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),  # Makes output shape fixed\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e79ebc-1df9-47c4-88b9-f3f91b09705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.float().to(device), labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.round().to(torch.uint8)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a1b8fce-291f-4dc9-bce8-addd73f0487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    for images, labels in dataloader:\n",
    "        images = images.float().to(device)\n",
    "        outputs = model(images).squeeze(-1)\n",
    "        preds = outputs.round().to(torch.uint8)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dada50a-2107-4c1d-b325-fae52c49f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4049b3f7-2491-466a-8776-ca376f3d325f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.7662346618516105, accuracy=0.4080717488789238\n",
      "Epoch 2: loss=0.6254737888063703, accuracy=0.4977578475336323\n",
      "Epoch 3: loss=0.6849923985345023, accuracy=0.6233183856502242\n",
      "Epoch 4: loss=0.6829503178596497, accuracy=0.5739910313901345\n",
      "Epoch 5: loss=0.6779274259294782, accuracy=0.5919282511210763\n",
      "Epoch 6: loss=0.6880688496998378, accuracy=0.5829596412556054\n",
      "Epoch 7: loss=0.6816920297486442, accuracy=0.5829596412556054\n",
      "Epoch 8: loss=0.669362621647971, accuracy=0.5739910313901345\n",
      "Epoch 9: loss=0.681648748261588, accuracy=0.5829596412556054\n",
      "Epoch 10: loss=0.6873103124754769, accuracy=0.57847533632287\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss, accuracy = train(model, dl_train, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}: {loss=}, {accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf30c8-5665-4da7-82e7-2dfee17d5e9c",
   "metadata": {},
   "source": [
    "## Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b946c82b-671d-462f-8836-e55c82cc4521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, dl_test, device)\n",
    "ids = [int(img.stem) for img in ds_test.images]\n",
    "\n",
    "subtask3_rows = []\n",
    "for id_, pred in zip(ids, predictions):\n",
    "    subtask3_rows.append((3, id_, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3470a0-3616-45e5-904b-4f14e508ec39",
   "metadata": {},
   "source": [
    "## Save answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7df874c-280b-473f-a4fc-727df57e33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rows = subtask1_rows + subtask2_rows + subtask3_rows\n",
    "df_submission = pd.DataFrame(submission_rows, columns=[\"subtaskID\", \"datapointID\", \"answer\"])\n",
    "df_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac409f0-5b7d-4b2a-bb48-e8232d4f5a51",
   "metadata": {},
   "source": [
    "## Submission results\n",
    "\n",
    "Subtask 1:\n",
    "- Accuracy: 1\n",
    "- Score: 10/10\n",
    "\n",
    "Subtask 2:\n",
    "- Accuracy: 1\n",
    "- Score: 10/10\n",
    "\n",
    "Subtask 3:\n",
    "- Accuracy: 0.529914\n",
    "- Score: 40/80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
